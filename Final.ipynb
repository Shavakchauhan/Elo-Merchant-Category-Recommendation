{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDYZGlHjqNWo"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SZ7foRxCptxG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import datetime as dt\n",
        "from google.colab import files\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import seaborn as sns\n",
        "import gc\n",
        "import lightgbm as lgb\n",
        "import json\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "import pickle\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connecting to google drive\n",
        "#Taken help from https://stackoverflow.com/questions/69822304/google-colab-google-drive-can%c2%b4t-be-mounted-anymore-browser-popup-google-dri\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "%cd /content\n",
        "!mkdir drive\n",
        "%cd drive\n",
        "!mkdir MyDrive\n",
        "%cd ..\n",
        "%cd ..\n",
        "!google-drive-ocamlfuse /content/drive/MyDrive\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7xcBcxSkqMt",
        "outputId": "02e17f17-b40e-4ecc-d5a2-465a7e35524c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 155113 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.27-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.27-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.27-0ubuntu1~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n",
            "/content\n",
            "/content/drive\n",
            "/content\n",
            "/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXiveBEHx6YB"
      },
      "source": [
        "# Downloading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Nd5bpbMuxzkq"
      },
      "outputs": [],
      "source": [
        "def download_data():\n",
        "  '''This function downloads the data into google colab and set up everythiing to process further with data.\n",
        "  Only things is you need to upload kaggle.json file which contains the user credential to letting api download data from kaggle\n",
        "  '''\n",
        "  # Requirement for downloading data directly to google colab\n",
        "  files.upload()\n",
        "  ! pip install kaggle\n",
        "  ! mkdir ~/.kaggle\n",
        "  ! cp kaggle.json ~/.kaggle/\n",
        "  ! chmod 600 ~/.kaggle/kaggle.json\n",
        "  # Downloading data to google colab\n",
        "  !kaggle competitions download -c elo-merchant-category-recommendation\n",
        "  # unzipping the required files\n",
        "  !unzip new_merchant_transactions.csv.zip\n",
        "  !unzip sample_submission.csv.zip\n",
        "  !unzip merchants.csv.zip\n",
        "  !unzip test.csv.zip\n",
        "  !unzip train.csv.zip\n",
        "  !unzip historical_transactions.csv.zip\n",
        "  # Deleting useless old zip files\n",
        "  !rm new_merchant_transactions.csv.zip\n",
        "  !rm sample_submission.csv.zip\n",
        "  !rm merchants.csv.zip\n",
        "  !rm test.csv.zip\n",
        "  !rm train.csv.zip\n",
        "  !rm historical_transactions.csv.zip\n",
        "  !mv new_merchant_transactions.csv /content/drive/MyDrive/appliedai/assignment/case_study_1/real_case_study_part\n",
        "  !mv sample_submission.csv /content/drive/MyDrive/appliedai/assignment/case_study_1/real_case_study_part\n",
        "  !mv merchants.csv /content/drive/MyDrive/appliedai/assignment/case_study_1/real_case_study_part\n",
        "  !mv test.csv /content/drive/MyDrive/appliedai/assignment/case_study_1/real_case_study_part\n",
        "  !mv train.csv /content/drive/MyDrive/appliedai/assignment/case_study_1/real_case_study_part\n",
        "  !mv historical_transactions.csv /content/drive/MyDrive/appliedai/assignment/case_study_1/real_case_study_part\n",
        "  os.chdir('/content/drive/MyDrive/appliedai/assignment/case_study_1/real_case_study_part')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "NH7EXuoZyXxI",
        "outputId": "17de7fe8-d27a-4c62-f6ee-f2a8d82a3bfe"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-737e0ae2-2566-4ee2-be0c-9bb0d68faff2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-737e0ae2-2566-4ee2-be0c-9bb0d68faff2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading train.csv.zip to /\n",
            "  0% 0.00/3.02M [00:00<?, ?B/s]\n",
            "100% 3.02M/3.02M [00:00<00:00, 102MB/s]\n",
            "Downloading test.csv.zip to /\n",
            "  0% 0.00/1.13M [00:00<?, ?B/s]\n",
            "100% 1.13M/1.13M [00:00<00:00, 157MB/s]\n",
            "Downloading new_merchant_transactions.csv.zip to /\n",
            " 77% 38.0M/49.4M [00:00<00:00, 60.2MB/s]\n",
            "100% 49.4M/49.4M [00:00<00:00, 142MB/s] \n",
            "Downloading historical_transactions.csv.zip to /\n",
            " 95% 523M/548M [00:02<00:00, 220MB/s]\n",
            "100% 548M/548M [00:02<00:00, 203MB/s]\n",
            "Downloading sample_submission.csv.zip to /\n",
            "  0% 0.00/846k [00:00<?, ?B/s]\n",
            "100% 846k/846k [00:00<00:00, 119MB/s]\n",
            "Downloading merchants.csv.zip to /\n",
            "  0% 0.00/12.7M [00:00<?, ?B/s]\n",
            "100% 12.7M/12.7M [00:00<00:00, 117MB/s]\n",
            "Downloading Data%20Dictionary.xlsx to /\n",
            "  0% 0.00/17.2k [00:00<?, ?B/s]\n",
            "100% 17.2k/17.2k [00:00<00:00, 18.2MB/s]\n",
            "Downloading Data_Dictionary.xlsx to /\n",
            "  0% 0.00/17.2k [00:00<?, ?B/s]\n",
            "100% 17.2k/17.2k [00:00<00:00, 17.5MB/s]\n",
            "Archive:  new_merchant_transactions.csv.zip\n",
            "  inflating: new_merchant_transactions.csv  \n",
            "Archive:  sample_submission.csv.zip\n",
            "  inflating: sample_submission.csv   \n",
            "Archive:  merchants.csv.zip\n",
            "  inflating: merchants.csv           \n",
            "Archive:  test.csv.zip\n",
            "  inflating: test.csv                \n",
            "Archive:  train.csv.zip\n",
            "  inflating: train.csv               \n",
            "Archive:  historical_transactions.csv.zip\n",
            "  inflating: historical_transactions.csv  \n"
          ]
        }
      ],
      "source": [
        "download_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM-tdkIrqhNd"
      },
      "source": [
        "# Train and test data processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM_5vVRr1F6K"
      },
      "source": [
        "Here time series data is provided in the train and test dataset and it can not be directly used thats why there are many features derived from the timeseries data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY-9XUMN1gYH"
      },
      "source": [
        "The reason for creating this festival features is that normally people purchase behaviour changes around festival therefore to see it influence on loyalty we added these features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tu1FyUYHz1o0"
      },
      "outputs": [],
      "source": [
        "def preprocessing_and_feature_engineering_train_and_test_data():\n",
        "  '''This function performs preprocessing and then does adds new features(feature engineering) \n",
        "  on train and test data by taking train and test data as input'''\n",
        "  train = pd.read_csv('train.csv',parse_dates=['first_active_month'])\n",
        "  test = pd.read_csv('test.csv',parse_dates=['first_active_month'])\n",
        "\n",
        "  train['days'] = (dt.date(2022,1,18)-pd.to_datetime(train['first_active_month']).dt.date).dt.days\n",
        "  train['quarter'] = pd.to_datetime(train['first_active_month']).dt.quarter\n",
        "  test['days'] = (dt.date(2022,1,18)-pd.to_datetime(test['first_active_month']).dt.date).dt.days\n",
        "  test['quarter'] = pd.to_datetime(test['first_active_month']).dt.quarter\n",
        "  train['date_difference_from_today']=abs(dt.date.today() - pd.to_datetime(train['first_active_month']).dt.date)\n",
        "  train['date_difference_from_today'] = train['date_difference_from_today'].map(lambda x: np.nan if pd.isnull(x) else x.days)\n",
        "  test['date_difference_from_today']=abs(dt.date.today() - pd.to_datetime(test['first_active_month']).dt.date)\n",
        "  test['date_difference_from_today'] = test['date_difference_from_today'].map(lambda x: np.nan if pd.isnull(x) else x.days)\n",
        "  train['day_of_week'] = pd.to_datetime(train['first_active_month']).dt.weekday\n",
        "  train['year'] = pd.to_datetime(train['first_active_month']).dt.year\n",
        "  test['day_of_week'] = pd.to_datetime(test['first_active_month']).dt.weekday\n",
        "  test['year'] = pd.to_datetime(test['first_active_month']).dt.year\n",
        "  train['month_of_year_first_active_month'] = pd.to_datetime(train['first_active_month']).dt.month\n",
        "  test['month_of_year_first_active_month'] = pd.to_datetime(test['first_active_month']).dt.month\n",
        "  train['month_start'] = pd.to_datetime(train['first_active_month']).dt.is_month_start\n",
        "  test['month_start'] = pd.to_datetime(test['first_active_month']).dt.is_month_start\n",
        "  train['day_of_year'] = pd.to_datetime(train['first_active_month']).dt.dayofyear\n",
        "  test['day_of_year'] = pd.to_datetime(test['first_active_month']).dt.dayofyear\n",
        "\n",
        "    # Inspired from https://www.kaggle.com/roydatascience/elo-merchant-recommendation-fathers-day-specials\n",
        "  #if purchase made in 60 days it is considered as influence\n",
        "  train['Christmas_Day_2017'] = (pd.to_datetime('2017-12-25') - pd.to_datetime(train['first_active_month'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "  train['Children_day_2017'] = (pd.to_datetime('2017-10-12') - pd.to_datetime(train['first_active_month'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "  train['Black_Friday_2017'] = (pd.to_datetime('2017-11-24') - pd.to_datetime(train['first_active_month'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "  train['Valentine_day_2017'] = (pd.to_datetime('2017-06-12') - pd.to_datetime(train['first_active_month'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "  train['Mothers_Day_2018'] = (pd.to_datetime('2018-05-13') - pd.to_datetime(train['first_active_month'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "  train['fathers_day_2017'] = (pd.to_datetime('2017-08-13') - pd.to_datetime(train['first_active_month'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "\n",
        "  test['Christmas_Day_2017'] = (pd.to_datetime('2017-12-25') - pd.to_datetime(test['first_active_month'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "  test['Children_day_2017'] = (pd.to_datetime('2017-10-12') - pd.to_datetime(test['first_active_month'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "  test['Black_Friday_2017'] = (pd.to_datetime('2017-11-24') - pd.to_datetime(test['first_active_month'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "  test['Valentine_day_2017'] = (pd.to_datetime('2017-06-12') - pd.to_datetime(test['first_active_month'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "  test['Mothers_Day_2018'] = (pd.to_datetime('2018-05-13') - pd.to_datetime(test['first_active_month'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "  test['fathers_day_2017'] = (pd.to_datetime('2017-08-13') - pd.to_datetime(test['first_active_month'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "\n",
        "  train['outliers'] = 0\n",
        "  train.loc[train['target'] < -30, 'outliers'] = 1\n",
        "\n",
        "  # Inspired from \n",
        "  for i in ['feature_1','feature_2','feature_3']:\n",
        "    train['days_'+i] = train['days']*train[i]\n",
        "    train['days_'+i + '_ratio'] = train[i]/train['days']\n",
        "\n",
        "  for i in ['feature_1','feature_2','feature_3']:\n",
        "    test['days_'+i] = test['days']*test[i]\n",
        "    test['days_'+i + '_ratio'] = test[i]/test['days']\n",
        "\n",
        "  gc.collect()\n",
        "  return train,test\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63aky75oqltS"
      },
      "source": [
        "# Historical transaction data processing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part we going to do preprocessing for historical transaction and adding some new features"
      ],
      "metadata": {
        "id": "x0jH8ngzRcS9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Uv7z5zFerLYR"
      },
      "outputs": [],
      "source": [
        "# Inspired from https://www.kaggle.com/c/elo-merchant-category-recommendation/discussion/82055#:~:text=def%20word2vec_feature(prefix,shape))%0A%20%20%20%20return%20df_w2v\n",
        "def word2vec_feature(prefix, df, groupby, target,size):\n",
        "    '''Converts the categorcal columns to vector form through word_2_vec technique\n",
        "    '''\n",
        "    df_bag = pd.DataFrame(df[[groupby, target]])\n",
        "    df_bag[target] = df_bag[target].astype(str)\n",
        "    df_bag[target].fillna('NAN', inplace=True)\n",
        "    df_bag = df_bag.groupby(groupby, as_index=False)[target].agg({'list':(lambda x: list(x))}).reset_index()\n",
        "    doc_list = list(df_bag['list'].values)\n",
        "    w2v = Word2Vec(doc_list, size=size, window=3, min_count=1, workers=32)\n",
        "    vocab_keys = list(w2v.wv.vocab.keys())\n",
        "    w2v_array = []\n",
        "    for v in vocab_keys :\n",
        "        w2v_array.append(list(w2v.wv[v]))\n",
        "    df_w2v = pd.DataFrame()\n",
        "    df_w2v['vocab_keys'] = vocab_keys    \n",
        "    df_w2v = pd.concat([df_w2v, pd.DataFrame(w2v_array)], axis=1)\n",
        "    df_w2v.columns = [target] + ['w2v_%s_%s_%d'%(prefix,target,x) for x in range(size)]\n",
        "    try:\n",
        "      del w2v_array,vocab_keys,doc_list,df_bag,prefix, df, groupby, target,size\n",
        "    except:\n",
        "      pass\n",
        "    return df_w2v\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rZWv_6YZr6Um"
      },
      "outputs": [],
      "source": [
        "def categorical_column_of_historical_transaction_with_word2vec(historical_transaction):\n",
        "  '''This function add word 2 vec vectors for categorical columns for historical transactions table\n",
        "  '''\n",
        "  historical_transaction['card_id_2'] = historical_transaction['card_id']\n",
        "  historical_transaction['purchase_date_only'] = pd.to_datetime(historical_transaction['purchase_date']).dt.date\n",
        "  card_id_word_2_vec = word2vec_feature('hist_data',historical_transaction,'card_id','card_id_2',5)\n",
        "  merchant_category_id_word_2_vec = word2vec_feature('hist_data',historical_transaction,'card_id','merchant_category_id',3)\n",
        "  subsector_id_word_2_vec = word2vec_feature('hist_data',historical_transaction,'card_id','subsector_id',3)\n",
        "  month_lag_word_2_vec = word2vec_feature('hist_data',historical_transaction,'card_id','month_lag',3)\n",
        "  purchase_date_word_2_vec = word2vec_feature('hist_data',historical_transaction,'card_id','purchase_date_only',5)\n",
        "  card_id_word_2_vec.rename(columns = {'card_id_2':'card_id'}, inplace = True)\n",
        "  historical_transaction = pd.merge(historical_transaction,card_id_word_2_vec,on='card_id',how='left')\n",
        "  try:\n",
        "    del card_id_word_2_vec\n",
        "  except:\n",
        "    pass\n",
        "  merchant_category_id_word_2_vec['merchant_category_id'] = merchant_category_id_word_2_vec['merchant_category_id'].astype('int')\n",
        "  historical_transaction = pd.merge(historical_transaction,merchant_category_id_word_2_vec,on='merchant_category_id',how='left')\n",
        "  try:\n",
        "    del merchant_category_id_word_2_vec\n",
        "  except:\n",
        "    pass\n",
        "  subsector_id_word_2_vec['subsector_id'] = subsector_id_word_2_vec['subsector_id'].astype('int')\n",
        "  historical_transaction = pd.merge(historical_transaction,subsector_id_word_2_vec,on='subsector_id',how='left')\n",
        "  try:\n",
        "    del subsector_id_word_2_vec\n",
        "  except:\n",
        "    pass\n",
        "  month_lag_word_2_vec['month_lag'] = month_lag_word_2_vec['month_lag'].astype('int')\n",
        "  historical_transaction = pd.merge(historical_transaction,month_lag_word_2_vec,on='month_lag',how='left')\n",
        "  try:\n",
        "    del month_lag_word_2_vec\n",
        "  except:\n",
        "    pass\n",
        "  purchase_date_word_2_vec['purchase_date_only'] = pd.to_datetime(purchase_date_word_2_vec['purchase_date_only']).dt.date\n",
        "  historical_transaction = pd.merge(historical_transaction,purchase_date_word_2_vec,on='purchase_date_only',how='left')\n",
        "  try:\n",
        "    del purchase_date_word_2_vec\n",
        "  except:\n",
        "    pass\n",
        "  return historical_transaction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting some of categorical features to vectors through word_2_vec form"
      ],
      "metadata": {
        "id": "wvgfJ-bBQ7LD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XvD_tC_RulGS"
      },
      "outputs": [],
      "source": [
        "def aggregation_historical_transaction(historical_transaction):\n",
        "  '''Aggregation function for historical transaction table'''\n",
        "  agg_policy_for_historical_transaction = {\n",
        "    'authorized_flag':['mean','sum'],\n",
        "    'city_id':['median','count','max'],\n",
        "    'category_1':['mean','sum','count','size'],\n",
        "    'installments':['mean','sum','std','min','max','var'],\n",
        "    'category_3':['mean','count','size','sum'],\n",
        "    'merchant_category_id':['median','nunique','min','max'],\n",
        "    'month_lag':['mean','sum','std','min','max','count','size','var'],\n",
        "    'purchase_amount':['mean','sum','std','min','max'],\n",
        "    'category_2':['mean','sum','max','count','size'],\n",
        "    'state_id':['nunique'],\n",
        "    'subsector_id':['nunique'],\n",
        "    'purchase_date_difference_from_today':['mean','median','min','max'],\n",
        "    'purchase_date_week_of_year':['nunique','min','max'],\n",
        "    'purchase_date_month':['nunique','sum'],\n",
        "    'purchase_date_day_of_week':['max','sum','nunique'],\n",
        "    'purchase_date_year':['nunique','sum'],\n",
        "    'Christmas_Day_2017':['mean','max','nunique','count','size'],\n",
        "    'fathers_day_2017':['mean','max','nunique','count','size'],\n",
        "    'Children_day_2017':['mean','max','nunique','count','size'],\n",
        "    'Black_Friday_2017':['mean','max','nunique','count','size'],\n",
        "    'Valentine_day_2017':['mean','max','nunique','count','size'],\n",
        "    'merchant_id':['nunique'],\n",
        "    'card_id':['size','count'],\n",
        "    'days':['mean','max','nunique','count','size'],\n",
        "    'quarter':['mean','max','nunique','count','size'],\n",
        "    'w2v_hist_data_card_id_2_0':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_card_id_2_1':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_card_id_2_2':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_card_id_2_3':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_card_id_2_4':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_merchant_category_id_0':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_merchant_category_id_1':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_merchant_category_id_2':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_subsector_id_0':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_subsector_id_1':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_subsector_id_2':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_month_lag_0':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_month_lag_1':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_month_lag_2':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_purchase_date_only_0':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_purchase_date_only_1':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_purchase_date_only_2':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_purchase_date_only_3':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_purchase_date_only_4':['sum','min','max','std','mean'],\n",
        "    'purchase_date' : ['max','min'],\n",
        "    'month_lag' : ['max','min','mean','var','skew'],\n",
        "    'month_diff' : ['max','min','mean','var','skew'],\n",
        "    'duration' : ['mean','min','max','var','skew'],\n",
        "    'amount_month_ratio':['mean','min','max','var','skew'],\n",
        "    'price' :['sum','mean','max','min','var'],\n",
        "    }\n",
        "\n",
        "  aggs_table_historical_transactions = historical_transaction.groupby(['card_id']).agg(agg_policy_for_historical_transaction)\n",
        "  aggs_table_historical_transactions.columns = ['hist_data_'+'_'.join(col).strip() for col in aggs_table_historical_transactions.columns.values]\n",
        "  try:\n",
        "    historical_transaction\n",
        "  except:\n",
        "    pass\n",
        "  return aggs_table_historical_transactions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above is aggregation function which aggregates the features in historical transaction table to its respective aggregable form."
      ],
      "metadata": {
        "id": "TgeSYWpnR5ab"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Iztr07spZhBb"
      },
      "outputs": [],
      "source": [
        "def preprocessing_and_feature_engineering_historical_data():\n",
        "  '''Processing and feature engineering for historical transaction data \n",
        "  '''\n",
        "  historical_transaction = pd.read_csv('historical_transactions.csv',parse_dates=['purchase_date'])\n",
        "  #impute missing values\n",
        "  historical_transaction['category_3'] = historical_transaction['category_3'].fillna('A',inplace=True)\n",
        "  historical_transaction['category_2'] = historical_transaction['category_2'].fillna(1.0,inplace=True)\n",
        "  historical_transaction['merchant_id'] = historical_transaction['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
        "  historical_transaction['installments'].replace(-1, np.nan,inplace=True)\n",
        "  historical_transaction['installments'].replace(999, np.nan,inplace=True)\n",
        "  historical_transaction['purchase_amount'] = historical_transaction['purchase_amount'].apply(lambda x: min(x, 0.8))\n",
        "  historical_transaction['authorized_flag'] = historical_transaction['authorized_flag'].map({'Y':1,'N':0})\n",
        "  historical_transaction['category_1'] = historical_transaction['category_1'].map({'Y':1,'N':0})\n",
        "\n",
        "  historical_transaction['category_3'] = historical_transaction['category_3'].map({'A':1,'B':2,'C':3})\n",
        "  historical_transaction['purchase_date_difference_from_today']=abs(dt.date.today() - pd.to_datetime(historical_transaction['purchase_date']).dt.date)\n",
        "  historical_transaction['purchase_date_difference_from_today'] = historical_transaction['purchase_date_difference_from_today'].map(lambda x: np.nan if pd.isnull(x) else x.days)\n",
        "  historical_transaction['purchase_date_week_of_year'] = pd.to_datetime(historical_transaction['purchase_date']).dt.isocalendar().week\n",
        "  historical_transaction['purchase_date_month'] = pd.to_datetime(historical_transaction['purchase_date']).dt.month\n",
        "  historical_transaction['purchase_date_day_of_week'] = pd.to_datetime(historical_transaction['purchase_date']).dt.weekday\n",
        "  historical_transaction['purchase_date_year'] = pd.to_datetime(historical_transaction['purchase_date']).dt.year\n",
        "  historical_transaction['days'] = (dt.date(2022,1,18)-pd.to_datetime(historical_transaction['purchase_date']).dt.date).dt.days\n",
        "  historical_transaction['quarter'] = pd.to_datetime(historical_transaction['purchase_date']).dt.quarter\n",
        "\n",
        "  # Inspired from https://www.kaggle.com/roydatascience/elo-stack-with-goss-boosting\n",
        "  historical_transaction['month_diff'] = historical_transaction['days']//30\n",
        "  historical_transaction['month_diff'] += historical_transaction['month_lag']\n",
        "  historical_transaction['duration'] = historical_transaction['purchase_amount']*historical_transaction['month_diff']\n",
        "  historical_transaction['amount_month_ratio'] = historical_transaction['purchase_amount']/historical_transaction['month_diff']\n",
        "  historical_transaction['price'] = historical_transaction['purchase_amount'] / historical_transaction['installments']\n",
        "\n",
        "  # Inspired from https://www.kaggle.com/roydatascience/elo-merchant-recommendation-fathers-day-specials\n",
        "  #if purchase made in 60 days it is considered as influence\n",
        "  historical_transaction['Christmas_Day_2017'] = (pd.to_datetime('2017-12-25') - pd.to_datetime(historical_transaction['purchase_date'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "  historical_transaction['Children_day_2017'] = (pd.to_datetime('2017-10-12') - pd.to_datetime(historical_transaction['purchase_date'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "  historical_transaction['Black_Friday_2017'] = (pd.to_datetime('2017-11-24') - pd.to_datetime(historical_transaction['purchase_date'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "  historical_transaction['Valentine_day_2017'] = (pd.to_datetime('2017-06-12') - pd.to_datetime(historical_transaction['purchase_date'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "  historical_transaction['Mothers_Day_2018'] = (pd.to_datetime('2018-05-13') - pd.to_datetime(historical_transaction['purchase_date'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "  historical_transaction['fathers_day_2017'] = (pd.to_datetime('2017-08-13') - pd.to_datetime(historical_transaction['purchase_date'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "\n",
        "  historical_transaction = categorical_column_of_historical_transaction_with_word2vec(historical_transaction)\n",
        "  gc.collect()\n",
        "  aggs_table_historical_transactions  = aggregation_historical_transaction(historical_transaction)\n",
        "  gc.collect()\n",
        "  return aggs_table_historical_transactions\n",
        "\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above is preprocessing and adding some new features for historical transaction .some features are related to time.Some are influnece on sales due to festival and last part is aggregating all these features."
      ],
      "metadata": {
        "id": "wg9xSveASO_E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "amvl6jdIxGP0"
      },
      "outputs": [],
      "source": [
        "def merging_train_test_with_historical_transaction(train,test,aggs_table_historical_transactions):\n",
        "  '''Preparing full train and test dataset by merging from aggragation tables.\n",
        "  '''\n",
        "  train = pd.merge(train,aggs_table_historical_transactions,on='card_id',how='left')\n",
        "  test = pd.merge(test,aggs_table_historical_transactions,on='card_id',how='left')\n",
        "  try:\n",
        "    del aggs_table_historical_transactions\n",
        "  except:\n",
        "    pass\n",
        "  gc.collect()\n",
        "  # Referrred from https://www.kaggle.com/roydatascience/elo-stack-with-goss-boosting\n",
        "  train['hist_purchase_date_max'] = pd.to_datetime(train['hist_data_purchase_date_max'])\n",
        "  train['hist_purchase_date_min'] = pd.to_datetime(train['hist_data_purchase_date_min'])\n",
        "  train['hist_purchase_date_diff'] = (train['hist_purchase_date_max'] - train['hist_purchase_date_min']).dt.days\n",
        "  train['hist_purchase_date_average'] = train['hist_purchase_date_diff']/train['hist_data_card_id_size']\n",
        "  train['hist_purchase_date_uptonow'] = (datetime.datetime.today() - train['hist_purchase_date_max']).dt.days\n",
        "  train['hist_purchase_date_uptomin'] = (datetime.datetime.today() - train['hist_purchase_date_min']).dt.days\n",
        "  train['hist_first_buy'] = (train['hist_data_purchase_date_min'] - train['first_active_month']).dt.days\n",
        "  train['hist_last_buy'] = (train['hist_data_purchase_date_max'] - train['first_active_month']).dt.days\n",
        "\n",
        "  for feature in ['hist_data_purchase_date_max','hist_data_purchase_date_min']:\n",
        "      train[feature] = train[feature].astype(np.int64) * 1e-9\n",
        "\n",
        "  test['hist_purchase_date_max'] = pd.to_datetime(test['hist_data_purchase_date_max'])\n",
        "  test['hist_purchase_date_min'] = pd.to_datetime(test['hist_data_purchase_date_min'])\n",
        "  test['hist_purchase_date_diff'] = (test['hist_purchase_date_max'] - test['hist_purchase_date_min']).dt.days\n",
        "  test['hist_purchase_date_average'] = test['hist_purchase_date_diff']/test['hist_data_card_id_size']\n",
        "  test['hist_purchase_date_uptonow'] = (datetime.datetime.today() - test['hist_purchase_date_max']).dt.days\n",
        "  test['hist_purchase_date_uptomin'] = (datetime.datetime.today() - test['hist_purchase_date_min']).dt.days\n",
        "  test['hist_first_buy'] = (test['hist_data_purchase_date_min'] - test['first_active_month']).dt.days\n",
        "  test['hist_last_buy'] = (test['hist_data_purchase_date_max'] - test['first_active_month']).dt.days\n",
        "\n",
        "  for feature in ['hist_data_purchase_date_max','hist_data_purchase_date_min']:\n",
        "      test[feature] = test[feature].astype(np.int64) * 1e-9\n",
        "  return train,test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above is merging of train and test with aggrgated form of historical transaction to further adding new features in the train and test table."
      ],
      "metadata": {
        "id": "IeJKXdSnSvSI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_5dnp-11im1"
      },
      "source": [
        "# Merchant Transaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uII45XXn1k8o"
      },
      "outputs": [],
      "source": [
        "def preprocessing_and_feature_engineering_merchants_data():\n",
        "  '''Preprocessing and feature engineering for merchants data\n",
        "  '''\n",
        "  merchants_data = pd.read_csv('merchants.csv')\n",
        "  merchants_data['category_1'] = merchants_data['category_1'].map({'Y':1,'N':0})\n",
        "  merchants_data['most_recent_sales_range'] = merchants_data['most_recent_sales_range'].map({'A':1,'B':2,'C':3,'D':4,'E':5})\n",
        "  merchants_data['most_recent_purchases_range'] = merchants_data['most_recent_purchases_range'].map({'A':1,'B':2,'C':3,'D':4,'E':5})\n",
        "  merchants_data['category_4'] = merchants_data['category_4'].map({'Y':1,'N':0})\n",
        "\n",
        "  merchants_data['merchant_id_2'] = merchants_data['merchant_id']\n",
        "  merchant_id_word_2_vec = word2vec_feature('merchant_data',merchants_data,'merchant_id','merchant_id_2',5)\n",
        "  merchant_id_word_2_vec.rename(columns = {'merchant_id_2':'merchant_id'}, inplace = True)\n",
        "  merchants_data.drop(['merchant_id_2'],axis=1,inplace=True)\n",
        "  merchants_data = pd.merge(merchants_data,merchant_id_word_2_vec,how='left',on='merchant_id')\n",
        "\n",
        "  merchants_data.columns = ['merchant_' + c if c not in ['merchant_id'] else c for c in merchants_data.columns ]\n",
        "  return merchants_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "preprocessing and transformation of some features for merchants data to make it ready to be used form."
      ],
      "metadata": {
        "id": "aM4jrGkRS93N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1H_gv-8F2ckQ"
      },
      "source": [
        "# New Merchant transaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PFKqUsJ23o1d"
      },
      "outputs": [],
      "source": [
        "def aggregation_merchants_data(merchants_data):\n",
        "  '''Aggregation function on merchants_data table\n",
        "  '''\n",
        "  aggs_merchants_data = {\n",
        "    'merchant_id':['nunique'],\n",
        "    'merchant_merchant_group_id':['nunique'],\n",
        "    'merchant_merchant_category_id':['nunique'],\n",
        "    'merchant_subsector_id':['nunique'],\n",
        "    'merchant_numerical_1':['mean','sum','std','min','max','var'],\n",
        "    'merchant_numerical_2':['mean','sum','std','min','max','var'],\n",
        "    'merchant_category_1':['mean','sum','min','max','count','nunique'],\n",
        "    'merchant_most_recent_sales_range':['sum','mean','min','max','var'],\n",
        "    'merchant_most_recent_purchases_range':['sum','mean','min','max','var'],\n",
        "    'merchant_avg_sales_lag3':['mean','std','min','max','count','nunique'],\n",
        "    'merchant_avg_purchases_lag3':['mean','std','min','max','count','nunique'],\n",
        "    'merchant_active_months_lag3':['min','std','max','count','nunique','count','nunique'],\n",
        "    'merchant_avg_sales_lag6':['mean','std','min','max','count','nunique'],\n",
        "    'merchant_avg_purchases_lag6':['mean','std','min','max','count','nunique'],\n",
        "    'merchant_active_months_lag6':['min','std','max','count','nunique'],\n",
        "    'merchant_avg_sales_lag12':['mean','std','min','max','count','nunique'],\n",
        "    'merchant_avg_purchases_lag12':['mean','std','min','max','count','nunique'],\n",
        "    'merchant_active_months_lag12':['min','std','max','count','nunique'],\n",
        "    'merchant_category_4':['mean','min','max','count','nunique','std'],\n",
        "    'merchant_city_id':['nunique'],\n",
        "    'merchant_state_id':['nunique'],\n",
        "    'merchant_category_2':['mean','sum','min','max','count','nunique'],\n",
        "    'authorized_flag':['mean','sum'],\n",
        "    'card_id':['size','count'],\n",
        "    'city_id':['nunique'],\n",
        "    'merchant_merchant_category_id':['nunique'],\n",
        "    'merchant_category_1':['sum','mean','max','min','count','nunique'],\n",
        "    'installments':['sum','std','min','max'],\n",
        "    'category_3':['sum','min','max','count','nunique'],\n",
        "    'merchant_merchant_category_id':['nunique'],\n",
        "    'month_lag':['min','max','sum','count'],\n",
        "    'purchase_amount':['min','max','sum','std','var','mean'],\n",
        "    'purchase_date':['max','min'],\n",
        "    'merchant_category_2':['sum','mean','max','min'],\n",
        "    'state_id':['nunique'],\n",
        "    'subsector_id':['nunique'],\n",
        "    'purchase_date_difference_from_today':['mean','min','max'],\n",
        "    'purchase_date_week_of_year':['nunique','min','max'],\n",
        "    'purchase_date_month':['nunique','min','max'],\n",
        "    'purchase_date_day_of_week':['nunique','min','max'],\n",
        "    'purchase_date_year':['nunique','min','max'],\n",
        "    'merchant_w2v_merchant_data_merchant_id_2_0':['sum','min','max','std'],\n",
        "    'merchant_w2v_merchant_data_merchant_id_2_1':['sum','min','max','std'],\n",
        "    'merchant_w2v_merchant_data_merchant_id_2_2':['sum','min','max','std'],\n",
        "    'merchant_w2v_merchant_data_merchant_id_2_3':['sum','min','max','std'],\n",
        "    'merchant_w2v_merchant_data_merchant_id_2_4':['sum','min','max','std'],\n",
        "    'month_lag' : ['max','min','mean','var','skew'],\n",
        "    'month_diff' : ['max','min','mean','var','skew'],\n",
        "    'duration' : ['mean','min','max','var','skew'],\n",
        "    'amount_month_ratio':['mean','min','max','var','skew'],\n",
        "    'price' :['sum','mean','max','min','var'],\n",
        "    'purchase_date':['min','max']\n",
        "    }\n",
        "  # Making the aggregation table for both merchants data and new merchants_data\n",
        "  agg_table_merchants_data = merchants_data.groupby('card_id').agg(aggs_merchants_data)\n",
        "  agg_table_merchants_data.columns = ['merchants_data_'+'_'.join(col).strip() for col in agg_table_merchants_data.columns.values]\n",
        "  return agg_table_merchants_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Appling aggrgation on the columns of merchants data and new merchants data to capture the interaction and make it usable for training further."
      ],
      "metadata": {
        "id": "dxectkSeTYYx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-4YSTexc2HzY"
      },
      "outputs": [],
      "source": [
        "def preprocessing_and_feature_engineering_new_merchant_transaction(merchants_data):\n",
        "  ''' Preprocessing data new merchants data and add some new features\n",
        "  '''\n",
        "  new_merchants_data = pd.read_csv('new_merchant_transactions.csv')\n",
        "  #impute missing values\n",
        "  new_merchants_data['category_2'].fillna(1.0,inplace=True)\n",
        "  new_merchants_data['category_3'].fillna('A',inplace=True)\n",
        "  new_merchants_data['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
        "  new_merchants_data['installments'].replace(-1, np.nan,inplace=True)\n",
        "  new_merchants_data['installments'].replace(999, np.nan,inplace=True)\n",
        "  new_merchants_data['purchase_amount'] = new_merchants_data['purchase_amount'].apply(lambda x: min(x, 0.8))\n",
        "\n",
        "  new_merchants_data['authorized_flag'] = new_merchants_data['authorized_flag'].map({'Y':1,'N':0})\n",
        "  new_merchants_data['category_1'] = new_merchants_data['category_1'].map({'Y': 1, 'N': 0})\n",
        "  new_merchants_data['category_3'] = new_merchants_data['category_3'].map({'A':1,'B':2,'C':3}) \n",
        "\n",
        "  new_merchants_data['purchase_date_difference_from_today']=abs(dt.date.today() - pd.to_datetime(new_merchants_data['purchase_date']).dt.date)\n",
        "  new_merchants_data['purchase_date_difference_from_today'] = new_merchants_data['purchase_date_difference_from_today'].map(lambda x: np.nan if pd.isnull(x) else x.days)\n",
        "  new_merchants_data['purchase_date_week_of_year'] = pd.to_datetime(new_merchants_data['purchase_date']).dt.isocalendar().week\n",
        "  new_merchants_data['purchase_date_month'] = pd.to_datetime(new_merchants_data['purchase_date']).dt.month\n",
        "  new_merchants_data['purchase_date_day_of_week'] = pd.to_datetime(new_merchants_data['purchase_date']).dt.weekday\n",
        "  new_merchants_data['purchase_date_year'] = pd.to_datetime(new_merchants_data['purchase_date']).dt.year\n",
        "  new_merchants_data['days'] = (dt.date(2022,1,18)-pd.to_datetime(new_merchants_data['purchase_date']).dt.date).dt.days\n",
        "  new_merchants_data['quarter'] = pd.to_datetime(new_merchants_data['purchase_date']).dt.quarter\n",
        "\n",
        "  # Inspired from https://www.kaggle.com/roydatascience/elo-stack-with-goss-boosting\n",
        "  new_merchants_data['month_diff'] = new_merchants_data['days']//30\n",
        "  new_merchants_data['month_diff'] += new_merchants_data['month_lag']\n",
        "  new_merchants_data['duration'] = new_merchants_data['purchase_amount']*new_merchants_data['month_diff']\n",
        "  new_merchants_data['amount_month_ratio'] = new_merchants_data['purchase_amount']/new_merchants_data['month_diff']\n",
        "  new_merchants_data['price'] = new_merchants_data['purchase_amount'] / new_merchants_data['installments']\n",
        "\n",
        "  \n",
        "  # Combining both merchants_data and new merchants_data\n",
        "  merchants_data = pd.merge(merchants_data,new_merchants_data,on='merchant_id',how='left')\n",
        "\n",
        "  try:\n",
        "    print('new merchants data table is deleted')\n",
        "    del new_merchants_data\n",
        "  except:\n",
        "    print('new merchants data table is already deleted')\n",
        "  \n",
        "  agg_table_merchants_data = aggregation_merchants_data(merchants_data)\n",
        "  try:\n",
        "    del merchants_data\n",
        "  except:\n",
        "    pass\n",
        "  return agg_table_merchants_data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merging the merchants data with new merchants data."
      ],
      "metadata": {
        "id": "lXJFvtjDTrCI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "72iWPxjC6DlJ"
      },
      "outputs": [],
      "source": [
        "def merging_train_test_with_merchant_transactions(train,test,agg_table_merchants_data):\n",
        "  '''Merges train and test data with merchants data and adds some news features as well'''\n",
        "  # Preparing full train and test dataset by merging from aggragation tables.\n",
        "  train = pd.merge(train,agg_table_merchants_data,on='card_id',how='left')\n",
        "  test = pd.merge(test,agg_table_merchants_data,on='card_id',how='left')\n",
        "  del agg_table_merchants_data\n",
        "  gc.collect()\n",
        "  # Refrrred from https://www.kaggle.com/roydatascience/elo-stack-with-goss-boosting\n",
        "  train['new_purchase_date_max'] = pd.to_datetime(train['merchants_data_purchase_date_year_max'])\n",
        "  train['new_purchase_date_min'] = pd.to_datetime(train['merchants_data_purchase_date_year_min'])\n",
        "  train['new_purchase_date_diff'] = (train['new_purchase_date_max'] - train['new_purchase_date_min']).dt.days\n",
        "  train['new_purchase_date_average'] = train['new_purchase_date_diff']/train['hist_data_card_id_size']\n",
        "  train['new_purchase_date_uptonow'] = (datetime.datetime.today() - train['new_purchase_date_max']).dt.days\n",
        "  train['new_purchase_date_uptomin'] = (datetime.datetime.today() - train['new_purchase_date_min']).dt.days\n",
        "  train['new_first_buy'] = (train['new_purchase_date_max'] - train['first_active_month']).dt.days\n",
        "  train['new_last_buy'] = (train['new_purchase_date_min'] - train['first_active_month']).dt.days\n",
        "\n",
        "  for feature in ['new_purchase_date_max','new_purchase_date_min']:\n",
        "      train[feature] = train[feature].astype(np.int64) * 1e-9\n",
        "\n",
        "  test['new_purchase_date_max'] = pd.to_datetime(test['merchants_data_purchase_date_year_max'])\n",
        "  test['new_purchase_date_min'] = pd.to_datetime(test['merchants_data_purchase_date_year_min'])\n",
        "  test['new_purchase_date_diff'] = (test['new_purchase_date_max'] - test['new_purchase_date_min']).dt.days\n",
        "  test['new_purchase_date_average'] = test['new_purchase_date_diff']/test['hist_data_card_id_size']\n",
        "  test['new_purchase_date_uptonow'] = (datetime.datetime.today() - test['new_purchase_date_max']).dt.days\n",
        "  test['new_purchase_date_uptomin'] = (datetime.datetime.today() - test['new_purchase_date_min']).dt.days\n",
        "  test['new_first_buy'] = (test['new_purchase_date_max'] - test['first_active_month']).dt.days\n",
        "  test['new_last_buy'] = (test['new_purchase_date_min'] - test['first_active_month']).dt.days\n",
        "\n",
        "  for feature in ['new_purchase_date_max','new_purchase_date_min']:\n",
        "      test[feature] = test[feature].astype(np.int64) * 1e-9\n",
        "  return train,test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "merging merchants data with train and test data."
      ],
      "metadata": {
        "id": "oCPaDmFlT5fp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UV3kO3F67dKx"
      },
      "outputs": [],
      "source": [
        "def extra_features_on_train_test(train,test):\n",
        "  '''This function adds new features on train and test\n",
        "  '''\n",
        "  #NEW Features referred from https://www.kaggle.com/mfjwr1/simple-lightgbm-without-blending\n",
        "  train['card_id_total'] = train['merchants_data_card_id_size']+train['hist_data_card_id_size']\n",
        "  train['card_id_cnt_total'] = train['merchants_data_card_id_count']+train['hist_data_card_id_count']\n",
        "  train['card_id_cnt_ratio'] = train['merchants_data_card_id_count']/train['hist_data_card_id_count']\n",
        "  train['purchase_amount_total'] = train['merchants_data_purchase_amount_sum']+train['hist_data_purchase_amount_sum']\n",
        "  train['purchase_amount_mean'] = train['merchants_data_purchase_amount_mean']+train['hist_data_purchase_amount_mean']\n",
        "  train['purchase_amount_max'] = train['merchants_data_purchase_amount_max']+train['hist_data_purchase_amount_max']\n",
        "  train['purchase_amount_min'] = train['merchants_data_purchase_amount_min']+train['hist_data_purchase_amount_min']\n",
        "  train['purchase_amount_ratio'] = train['merchants_data_purchase_amount_sum']/train['hist_data_purchase_amount_sum']\n",
        "  train['month_diff_mean'] = train['merchants_data_month_diff_mean']+train['hist_data_month_diff_mean']\n",
        "  train['month_diff_ratio'] = train['merchants_data_month_diff_mean']/train['hist_data_month_diff_mean']\n",
        "  train['month_lag_mean'] = train['merchants_data_month_lag_mean']+train['hist_data_month_lag_mean']\n",
        "  train['month_lag_max'] = train['merchants_data_month_lag_max']+train['hist_data_month_lag_max']\n",
        "  train['month_lag_min'] = train['merchants_data_month_lag_min']+train['hist_data_month_lag_min']\n",
        "  train['category_1_mean'] = train['merchants_data_merchant_category_1_mean']+train['hist_data_category_1_mean']\n",
        "  train['installments_total'] = train['merchants_data_installments_sum']+train['hist_data_installments_sum']\n",
        "  # train['installments_mean'] = train['merchants_data_installments_mean']+train['hist_data_installments_sum']\n",
        "  train['installments_max'] = train['merchants_data_installments_max']+train['hist_data_installments_max']\n",
        "  train['installments_ratio'] = train['merchants_data_installments_sum']/train['hist_data_installments_sum']\n",
        "  train['price_total'] = train['hist_data_purchase_amount_sum'] / train['hist_data_installments_sum']\n",
        "  train['price_mean'] = train['hist_data_purchase_amount_mean'] / train['hist_data_installments_mean']\n",
        "  train['price_max'] = train['hist_data_purchase_amount_max'] / train['hist_data_installments_max']\n",
        "  train['duration_mean'] = train['merchants_data_duration_mean']+train['hist_data_duration_mean']\n",
        "  train['duration_min'] = train['merchants_data_duration_min']+train['hist_data_duration_min']\n",
        "  train['duration_max'] = train['merchants_data_duration_max']+train['hist_data_duration_max']\n",
        "  train['amount_month_ratio_mean']=train['merchants_data_amount_month_ratio_mean']+train['hist_data_amount_month_ratio_mean']\n",
        "  train['amount_month_ratio_min']=train['merchants_data_amount_month_ratio_min']+train['hist_data_amount_month_ratio_min']\n",
        "  train['amount_month_ratio_max']=train['merchants_data_amount_month_ratio_max']+train['hist_data_amount_month_ratio_max']\n",
        "  train['new_CLV'] = train['merchants_data_card_id_count'] * train['merchants_data_purchase_amount_sum'] / train['merchants_data_month_diff_mean']\n",
        "  train['hist_CLV'] = train['hist_data_card_id_count'] * train['hist_data_purchase_amount_sum'] / train['hist_data_month_diff_mean']\n",
        "  train['CLV_ratio'] = train['new_CLV'] / train['hist_CLV']\n",
        "\n",
        "  test['card_id_total'] = test['merchants_data_card_id_size']+test['hist_data_card_id_size']\n",
        "  test['card_id_cnt_total'] = test['merchants_data_card_id_count']+test['hist_data_card_id_count']\n",
        "  test['card_id_cnt_ratio'] = test['merchants_data_card_id_count']/test['hist_data_card_id_count']\n",
        "  test['purchase_amount_total'] = test['merchants_data_purchase_amount_sum']+test['hist_data_purchase_amount_sum']\n",
        "  test['purchase_amount_mean'] = test['merchants_data_purchase_amount_mean']+test['hist_data_purchase_amount_mean']\n",
        "  test['purchase_amount_max'] = test['merchants_data_purchase_amount_max']+test['hist_data_purchase_amount_max']\n",
        "  test['purchase_amount_min'] = test['merchants_data_purchase_amount_min']+test['hist_data_purchase_amount_min']\n",
        "  test['purchase_amount_ratio'] = test['merchants_data_purchase_amount_sum']/test['hist_data_purchase_amount_sum']\n",
        "  test['month_diff_mean'] = test['merchants_data_month_diff_mean']+test['hist_data_month_diff_mean']\n",
        "  test['month_diff_ratio'] = test['merchants_data_month_diff_mean']/test['hist_data_month_diff_mean']\n",
        "  test['month_lag_mean'] = test['merchants_data_month_lag_mean']+test['hist_data_month_lag_mean']\n",
        "  test['month_lag_max'] = test['merchants_data_month_lag_max']+test['hist_data_month_lag_max']\n",
        "  test['month_lag_min'] = test['merchants_data_month_lag_min']+test['hist_data_month_lag_min']\n",
        "  test['category_1_mean'] = test['merchants_data_merchant_category_1_mean']+test['hist_data_category_1_mean']\n",
        "  test['installments_total'] = test['merchants_data_installments_sum']+test['hist_data_installments_sum']\n",
        "  # test['installments_mean'] = test['merchants_data_installments_mean']+test['hist_data_installments_sum']\n",
        "  test['installments_max'] = test['merchants_data_installments_max']+test['hist_data_installments_max']\n",
        "  test['installments_ratio'] = test['merchants_data_installments_sum']/test['hist_data_installments_sum']\n",
        "  test['price_total'] = test['hist_data_purchase_amount_sum'] / test['hist_data_installments_sum']\n",
        "  test['price_mean'] = test['hist_data_purchase_amount_mean'] / test['hist_data_installments_mean']\n",
        "  test['price_max'] = test['hist_data_purchase_amount_max'] / test['hist_data_installments_max']\n",
        "  test['duration_mean'] = test['merchants_data_duration_mean']+test['hist_data_duration_mean']\n",
        "  test['duration_min'] = test['merchants_data_duration_min']+test['hist_data_duration_min']\n",
        "  test['duration_max'] = test['merchants_data_duration_max']+test['hist_data_duration_max']\n",
        "  test['amount_month_ratio_mean']=test['merchants_data_amount_month_ratio_mean']+test['hist_data_amount_month_ratio_mean']\n",
        "  test['amount_month_ratio_min']=test['merchants_data_amount_month_ratio_min']+test['hist_data_amount_month_ratio_min']\n",
        "  test['amount_month_ratio_max']=test['merchants_data_amount_month_ratio_max']+test['hist_data_amount_month_ratio_max']\n",
        "  test['new_CLV'] = test['merchants_data_card_id_count'] * test['merchants_data_purchase_amount_sum'] / test['merchants_data_month_diff_mean']\n",
        "  test['hist_CLV'] = test['hist_data_card_id_count'] * test['hist_data_purchase_amount_sum'] / test['hist_data_month_diff_mean']\n",
        "  test['CLV_ratio'] = test['new_CLV'] / test['hist_CLV']\n",
        "\n",
        "  return train,test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above function add some new features mainly to boost the score of kaggle leaderboard."
      ],
      "metadata": {
        "id": "YvpIAC2WUAkE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UADrUxb070tX"
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "  '''Loads data with trainable form \n",
        "  '''\n",
        "  train,test = preprocessing_and_feature_engineering_train_and_test_data()\n",
        "  print('First train test finished')\n",
        "  historical_transaction = preprocessing_and_feature_engineering_historical_data()\n",
        "  print('historical transaction fininshed')\n",
        "  train,test = merging_train_test_with_historical_transaction(train,test,historical_transaction)\n",
        "  try:\n",
        "    del historical_transaction\n",
        "    print('Historical transaction with train test finished')\n",
        "  except:\n",
        "    pass\n",
        "  merchants_data = preprocessing_and_feature_engineering_merchants_data()\n",
        "  new_merchants_transaction = preprocessing_and_feature_engineering_new_merchant_transaction(merchants_data)\n",
        "  try:\n",
        "    del merchants_data\n",
        "    print('merchants transaction with train test finished')\n",
        "  except:\n",
        "    pass\n",
        "  train,test = merging_train_test_with_merchant_transactions(train,test,new_merchants_transaction)\n",
        "  try:\n",
        "    del new_merchants_transaction\n",
        "    print('new merchants transaction with train test finished')\n",
        "  except:\n",
        "    pass\n",
        "  train,test = extra_features_on_train_test(train,test)\n",
        "  train.drop(['first_active_month','merchants_data_purchase_date_min','merchants_data_purchase_date_max','hist_purchase_date_max','hist_purchase_date_min'],axis=1,inplace=True)\n",
        "  test.drop(['first_active_month','merchants_data_purchase_date_min','merchants_data_purchase_date_max','hist_purchase_date_max','hist_purchase_date_min'],axis=1,inplace=True)\n",
        "  features = [c for c in train.columns if c not in ['target','outliers','card_id']]\n",
        "  for i in features:\n",
        "    if np.any(np.isnan(train[i])):\n",
        "      train[i].fillna(train[i].median(),inplace=True)\n",
        "\n",
        "  for i in features:\n",
        "    if np.any(np.isnan(test[i])):\n",
        "      test[i].fillna(test[i].median(),inplace=True)\n",
        "  \n",
        "  return train,test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv7W01Aw9jjH",
        "outputId": "21d8da17-3bf0-458d-8125-b8169c6df33d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First train test finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.base_any2vec:consider setting layer size to a multiple of 4 for greater performance\n",
            "WARNING:gensim.models.base_any2vec:consider setting layer size to a multiple of 4 for greater performance\n",
            "WARNING:gensim.models.base_any2vec:consider setting layer size to a multiple of 4 for greater performance\n",
            "WARNING:gensim.models.base_any2vec:consider setting layer size to a multiple of 4 for greater performance\n",
            "WARNING:gensim.models.base_any2vec:consider setting layer size to a multiple of 4 for greater performance\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "historical transaction fininshed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Historical transaction with train test finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.base_any2vec:consider setting layer size to a multiple of 4 for greater performance\n",
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new merchants data table is deleted\n",
            "merchants transaction with train test finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new merchants transaction with train test finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py:6392: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  return self._update_inplace(result)\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py:6392: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  return self._update_inplace(result)\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py:6392: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  return self._update_inplace(result)\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py:6392: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  return self._update_inplace(result)\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py:6392: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  return self._update_inplace(result)\n"
          ]
        }
      ],
      "source": [
        "train,test = load_data()\n",
        "# Loading the train and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "tOzeZf2cGqxZ"
      },
      "outputs": [],
      "source": [
        "# Saving train and test dataframe to csv so that it can reused directly in future without having to execute above code\n",
        "train.to_csv('feature_engineered_train.csv')\n",
        "test.to_csv('feature_engineered_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('feature_engineered_train.csv')\n",
        "test = pd.read_csv('feature_engineered_test.csv')\n",
        "train.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
        "test.drop(['Unnamed: 0'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "l4T57i0V1BeL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predictions and error values"
      ],
      "metadata": {
        "id": "A2vBQeXLU18X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction_of_loyalty_score_for_single_value(X,train=train,test=test):\n",
        "  '''This model takes single values that is card id and output the loyalyty score for it\n",
        "  '''\n",
        "  # train,test = load_data() we cannot use load data because it takes to much time to execute approximately half hour\n",
        "  X = [X]\n",
        "  features_1 = [c for c in train.columns.values if c not in ['target','outliers']]\n",
        "  features = [c for c in train.columns.values if c not in ['card_id','target','outliers']]\n",
        "  train = pd.concat([train[features_1],test[features_1]],axis=0)\n",
        "  train = train.loc[train['card_id'].isin(X)]\n",
        "  with open('best_model_lightgbm.sav', 'rb') as pickle_file:\n",
        "    model = pickle.load(pickle_file)\n",
        "  prediction_lgb = model.predict(train[features] , num_iteration=model.best_iteration) \n",
        "  predcition_target = pd.DataFrame()\n",
        "  predcition_target['card_id'] = train['card_id'] \n",
        "  predcition_target['predicted_target'] = prediction_lgb\n",
        "  predcition_target.set_index('card_id',inplace=True)\n",
        "  return predcition_target\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VQT8Q1j4hhls"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will select any random card_id from the all the set of available cards and give it predicted output\n",
        "card_ids = pd.concat([train['card_id'],test['card_id']]).to_list()\n",
        "selected_card_id = card_ids[random.randint(0,len(card_ids)-1)]\n",
        "ans = prediction_of_loyalty_score_for_single_value(selected_card_id)\n",
        "ans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "1T7jgSyW0YGZ",
        "outputId": "4a6f8270-86eb-4d20-88ab-7d8df67d088e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-eb66c41a-7361-4ae2-9724-178e52131c61\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>predicted_target</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>card_id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>C_ID_fb375ca20d</th>\n",
              "      <td>-1.85484</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb66c41a-7361-4ae2-9724-178e52131c61')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-eb66c41a-7361-4ae2-9724-178e52131c61 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-eb66c41a-7361-4ae2-9724-178e52131c61');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                 predicted_target\n",
              "card_id                          \n",
              "C_ID_fb375ca20d          -1.85484"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rmse(y,y_pred):\n",
        "  ''' Gives root mean squred error between true and predicted values\n",
        "  '''\n",
        "  if len(y)==len(y_pred):\n",
        "    temp_sum = 0\n",
        "    for i in range(len(y)):\n",
        "      temp_sum+=abs(y[i]-y_pred[i])**2\n",
        "    temp_sum = (temp_sum/len(y))**0.5\n",
        "    return temp_sum\n",
        "  return 0"
      ],
      "metadata": {
        "id": "baR03kRAhZrx"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def function_2(X,y,train=train):\n",
        "  '''This function takes X and y as input and returns the dataframe with real target values and predicted \n",
        "  target values and also return rmse(root mean squre error) score\n",
        "  '''\n",
        "  features = [c for c in train.columns.values if c not in ['card_id','target','outliers']]\n",
        "  with open('best_model_lightgbm.sav', 'rb') as pickle_file:\n",
        "    model = pickle.load(pickle_file)\n",
        "  prediction_lgb = model.predict(X[features] , num_iteration=model.best_iteration) \n",
        "  prediction_df = pd.DataFrame()\n",
        "  prediction_df['card_id'] = X['card_id']\n",
        "  prediction_df['target'] = y\n",
        "  prediction_df['predicted_target'] = prediction_lgb\n",
        "  prediction_df.set_index('card_id',inplace=True)\n",
        "  rmse_score = rmse(list(prediction_df['target'].values),list(prediction_df['predicted_target'].values))\n",
        "  return prediction_df,rmse_score\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GqE1P7r__Uey"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_indexes = random.sample(range(0,train.shape[0]-1), 10)\n",
        "X = train.drop(['target','outliers'],axis=1).iloc[random_indexes]\n",
        "y= train['target'].iloc[random_indexes]\n",
        "predicted_dataFrame,rmse_score = function_2(X,y)\n",
        "print(predicted_dataFrame)\n",
        "print('='*100)\n",
        "print(rmse_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Aq9khOtDI9P",
        "outputId": "04444dd8-9e12-4762-c275-c2bd1806f351"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   target  predicted_target\n",
            "card_id                                    \n",
            "C_ID_e20f8acc96  0.825537         -3.003099\n",
            "C_ID_e643408052  0.580037         -2.023828\n",
            "C_ID_edcb0cdeb4 -2.719273         -2.979403\n",
            "C_ID_608e423f68 -0.120842         -2.013438\n",
            "C_ID_f75121c378  1.409248         -2.803894\n",
            "C_ID_8f5f7484a4  0.132801         -2.746789\n",
            "C_ID_1e15258bdd  2.343477         -2.356914\n",
            "C_ID_a79601d791 -0.316450         -2.371260\n",
            "C_ID_2d34870fa8 -1.696413         -2.911766\n",
            "C_ID_5090f139b8  0.348833         -2.872706\n",
            "====================================================================================================\n",
            "2.9883452221030145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ncgCua9NNwLA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "name": "Final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}