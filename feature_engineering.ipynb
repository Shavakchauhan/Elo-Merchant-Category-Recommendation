{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "feature_engineering.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Importing libraries"
      ],
      "metadata": {
        "id": "7HXJHLBzAiTr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vljDzxDQAP5n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import datetime as dt\n",
        "from google.colab import files\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import seaborn as sns\n",
        "import gc\n",
        "import lightgbm as lgb\n",
        "import json\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from tqdm import tqdm\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data Downloading and performing initial setup"
      ],
      "metadata": {
        "id": "TUyAI3ZxAu34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using kaggle.json (api method) file to directly download data to google colab\n",
        "files.upload()\n",
        "# Requirement for downloading data directly to google colab\n",
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "# Downloading data to google colab\n",
        "!kaggle competitions download -c elo-merchant-category-recommendation\n",
        "# unzipping the required files\n",
        "!unzip new_merchant_transactions.csv.zip\n",
        "!unzip sample_submission.csv.zip\n",
        "!unzip merchants.csv.zip\n",
        "!unzip test.csv.zip\n",
        "!unzip train.csv.zip\n",
        "!unzip historical_transactions.csv.zip\n",
        "# Deleting useless old zip files\n",
        "!rm new_merchant_transactions.csv.zip\n",
        "!rm sample_submission.csv.zip\n",
        "!rm merchants.csv.zip\n",
        "!rm test.csv.zip\n",
        "!rm train.csv.zip\n",
        "!rm historical_transactions.csv.zip"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 906
        },
        "id": "tr-rpTqxApu_",
        "outputId": "4fbbb5f2-3e58-467b-ef9b-fd6a85912fac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-59c77ee0-f7fb-44f7-b1fc-0cb44856d6f8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-59c77ee0-f7fb-44f7-b1fc-0cb44856d6f8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading Data%20Dictionary.xlsx to /content\n",
            "  0% 0.00/17.2k [00:00<?, ?B/s]\n",
            "100% 17.2k/17.2k [00:00<00:00, 14.3MB/s]\n",
            "Downloading Data_Dictionary.xlsx to /content\n",
            "  0% 0.00/17.2k [00:00<?, ?B/s]\n",
            "100% 17.2k/17.2k [00:00<00:00, 15.6MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            "  0% 0.00/3.02M [00:00<?, ?B/s]\n",
            "100% 3.02M/3.02M [00:00<00:00, 100MB/s]\n",
            "Downloading new_merchant_transactions.csv.zip to /content\n",
            " 67% 33.0M/49.4M [00:00<00:00, 29.3MB/s]\n",
            "100% 49.4M/49.4M [00:00<00:00, 56.6MB/s]\n",
            "Downloading merchants.csv.zip to /content\n",
            " 71% 9.00M/12.7M [00:00<00:00, 25.8MB/s]\n",
            "100% 12.7M/12.7M [00:00<00:00, 36.4MB/s]\n",
            "Downloading historical_transactions.csv.zip to /content\n",
            " 97% 529M/548M [00:10<00:00, 26.0MB/s]\n",
            "100% 548M/548M [00:10<00:00, 55.7MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/846k [00:00<?, ?B/s]\n",
            "100% 846k/846k [00:00<00:00, 121MB/s]\n",
            "Downloading test.csv.zip to /content\n",
            "  0% 0.00/1.13M [00:00<?, ?B/s]\n",
            "100% 1.13M/1.13M [00:00<00:00, 76.0MB/s]\n",
            "Archive:  new_merchant_transactions.csv.zip\n",
            "  inflating: new_merchant_transactions.csv  \n",
            "Archive:  sample_submission.csv.zip\n",
            "  inflating: sample_submission.csv   \n",
            "Archive:  merchants.csv.zip\n",
            "  inflating: merchants.csv           \n",
            "Archive:  test.csv.zip\n",
            "  inflating: test.csv                \n",
            "Archive:  train.csv.zip\n",
            "  inflating: train.csv               \n",
            "Archive:  historical_transactions.csv.zip\n",
            "  inflating: historical_transactions.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Taken from https://www.kaggle.com/rinnqd/reduce-memory-usage\n",
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)    \n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    return df"
      ],
      "metadata": {
        "id": "nD7SAgaiAwz2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Loading data"
      ],
      "metadata": {
        "id": "1WrBuxQiBICm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Train and test data"
      ],
      "metadata": {
        "id": "jAPfIOg1BKcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train.csv',parse_dates=['first_active_month'])\n",
        "test = pd.read_csv('test.csv',parse_dates=['first_active_month'])"
      ],
      "metadata": {
        "id": "AubdXQXgBGA1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['days'] = (dt.date(2022,1,18)-pd.to_datetime(train['first_active_month']).dt.date).dt.days\n",
        "train['quarter'] = pd.to_datetime(train['first_active_month']).dt.quarter\n",
        "test['days'] = (dt.date(2022,1,18)-pd.to_datetime(test['first_active_month']).dt.date).dt.days\n",
        "test['quarter'] = pd.to_datetime(test['first_active_month']).dt.quarter\n",
        "train['date_difference_from_today']=abs(dt.date.today() - pd.to_datetime(train['first_active_month']).dt.date)\n",
        "train['date_difference_from_today'] = train['date_difference_from_today'].map(lambda x: np.nan if pd.isnull(x) else x.days)\n",
        "test['date_difference_from_today']=abs(dt.date.today() - pd.to_datetime(test['first_active_month']).dt.date)\n",
        "test['date_difference_from_today'] = test['date_difference_from_today'].map(lambda x: np.nan if pd.isnull(x) else x.days)\n",
        "train['day_of_week'] = pd.to_datetime(train['first_active_month']).dt.weekday\n",
        "train['year'] = pd.to_datetime(train['first_active_month']).dt.year\n",
        "test['day_of_week'] = pd.to_datetime(test['first_active_month']).dt.weekday\n",
        "test['year'] = pd.to_datetime(test['first_active_month']).dt.year\n",
        "train['month_of_year_first_active_month'] = pd.to_datetime(train['first_active_month']).dt.month\n",
        "test['month_of_year_first_active_month'] = pd.to_datetime(test['first_active_month']).dt.month\n",
        "train['month_start'] = pd.to_datetime(train['first_active_month']).dt.is_month_start\n",
        "test['month_start'] = pd.to_datetime(test['first_active_month']).dt.is_month_start\n",
        "train['day_of_year'] = pd.to_datetime(train['first_active_month']).dt.dayofyear\n",
        "test['day_of_year'] = pd.to_datetime(test['first_active_month']).dt.dayofyear\n"
      ],
      "metadata": {
        "id": "Izv_LYncBTQv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspired from https://www.kaggle.com/roydatascience/elo-merchant-recommendation-fathers-day-specials\n",
        "#if purchase made in 60 days it is considered as influence\n",
        "train['Christmas_Day_2017'] = (pd.to_datetime('2017-12-25') - pd.to_datetime(train['first_active_month'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "train['Children_day_2017'] = (pd.to_datetime('2017-10-12') - pd.to_datetime(train['first_active_month'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "train['Black_Friday_2017'] = (pd.to_datetime('2017-11-24') - pd.to_datetime(train['first_active_month'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "train['Valentine_day_2017'] = (pd.to_datetime('2017-06-12') - pd.to_datetime(train['first_active_month'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "train['Mothers_Day_2018'] = (pd.to_datetime('2018-05-13') - pd.to_datetime(train['first_active_month'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "train['fathers_day_2017'] = (pd.to_datetime('2017-08-13') - pd.to_datetime(train['first_active_month'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "\n",
        "test['Christmas_Day_2017'] = (pd.to_datetime('2017-12-25') - pd.to_datetime(test['first_active_month'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "test['Children_day_2017'] = (pd.to_datetime('2017-10-12') - pd.to_datetime(test['first_active_month'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "test['Black_Friday_2017'] = (pd.to_datetime('2017-11-24') - pd.to_datetime(test['first_active_month'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "test['Valentine_day_2017'] = (pd.to_datetime('2017-06-12') - pd.to_datetime(test['first_active_month'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "test['Mothers_Day_2018'] = (pd.to_datetime('2018-05-13') - pd.to_datetime(test['first_active_month'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "test['fathers_day_2017'] = (pd.to_datetime('2017-08-13') - pd.to_datetime(test['first_active_month'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n"
      ],
      "metadata": {
        "id": "brzbXmhbBXp1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['outliers'] = 0\n",
        "train.loc[train['target'] < -30, 'outliers'] = 1"
      ],
      "metadata": {
        "id": "YVfvasQJBbkn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspired from \n",
        "for i in ['feature_1','feature_2','feature_3']:\n",
        "  train['days_'+i] = train['days']*train[i]\n",
        "  train['days_'+i + '_ratio'] = train[i]/train['days']\n",
        "\n",
        "for i in ['feature_1','feature_2','feature_3']:\n",
        "  test['days_'+i] = test['days']*test[i]\n",
        "  test['days_'+i + '_ratio'] = test[i]/test['days']\n",
        "\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIN_NnGxCEs7",
        "outputId": "45fa449a-4a88-4003-ab73-b3b4d5deaa96"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "117"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Historical Transactions"
      ],
      "metadata": {
        "id": "TYzDB1JcFmZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "historical_transaction = pd.read_csv('historical_transactions.csv',parse_dates=['purchase_date'])\n",
        "#impute missing values\n",
        "historical_transaction['category_2'] = historical_transaction['category_2'].fillna(1.0,inplace=True)\n",
        "historical_transaction['category_3'] = historical_transaction['category_3'].fillna('A',inplace=True)\n",
        "historical_transaction['merchant_id'] = historical_transaction['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
        "historical_transaction['installments'].replace(-1, np.nan,inplace=True)\n",
        "historical_transaction['installments'].replace(999, np.nan,inplace=True)\n",
        "historical_transaction['purchase_amount'] = historical_transaction['purchase_amount'].apply(lambda x: min(x, 0.8))\n",
        "\n",
        "historical_transaction['authorized_flag'] = historical_transaction['authorized_flag'].map({'Y':1,'N':0})\n",
        "historical_transaction['category_1'] = historical_transaction['category_1'].map({'Y':1,'N':0})\n",
        "historical_transaction['category_3'] = historical_transaction['category_3'].map({'A':1,'B':2,'C':3})\n",
        "historical_transaction['purchase_date_difference_from_today']=abs(dt.date.today() - pd.to_datetime(historical_transaction['purchase_date']).dt.date)\n",
        "historical_transaction['purchase_date_difference_from_today'] = historical_transaction['purchase_date_difference_from_today'].map(lambda x: np.nan if pd.isnull(x) else x.days)\n",
        "historical_transaction['purchase_date_week_of_year'] = pd.to_datetime(historical_transaction['purchase_date']).dt.isocalendar().week\n",
        "historical_transaction['purchase_date_month'] = pd.to_datetime(historical_transaction['purchase_date']).dt.month\n",
        "historical_transaction['purchase_date_day_of_week'] = pd.to_datetime(historical_transaction['purchase_date']).dt.weekday\n",
        "historical_transaction['purchase_date_year'] = pd.to_datetime(historical_transaction['purchase_date']).dt.year\n",
        "historical_transaction['days'] = (dt.date(2022,1,18)-pd.to_datetime(historical_transaction['purchase_date']).dt.date).dt.days\n",
        "historical_transaction['quarter'] = pd.to_datetime(historical_transaction['purchase_date']).dt.quarter"
      ],
      "metadata": {
        "id": "m-127dgGFIrf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspired from https://www.kaggle.com/roydatascience/elo-stack-with-goss-boosting\n",
        "historical_transaction['month_diff'] = historical_transaction['days']//30\n",
        "historical_transaction['month_diff'] += historical_transaction['month_lag']\n",
        "historical_transaction['duration'] = historical_transaction['purchase_amount']*historical_transaction['month_diff']\n",
        "historical_transaction['amount_month_ratio'] = historical_transaction['purchase_amount']/historical_transaction['month_diff']\n",
        "historical_transaction['price'] = historical_transaction['purchase_amount'] / historical_transaction['installments']\n"
      ],
      "metadata": {
        "id": "hOz7ZsJeIXU0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspired from https://www.kaggle.com/roydatascience/elo-merchant-recommendation-fathers-day-specials\n",
        "#if purchase made in 60 days it is considered as influence\n",
        "historical_transaction['Christmas_Day_2017'] = (pd.to_datetime('2017-12-25') - pd.to_datetime(historical_transaction['purchase_date'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "historical_transaction['Children_day_2017'] = (pd.to_datetime('2017-10-12') - pd.to_datetime(historical_transaction['purchase_date'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "historical_transaction['Black_Friday_2017'] = (pd.to_datetime('2017-11-24') - pd.to_datetime(historical_transaction['purchase_date'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "historical_transaction['Valentine_day_2017'] = (pd.to_datetime('2017-06-12') - pd.to_datetime(historical_transaction['purchase_date'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "historical_transaction['Mothers_Day_2018'] = (pd.to_datetime('2018-05-13') - pd.to_datetime(historical_transaction['purchase_date'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n",
        "historical_transaction['fathers_day_2017'] = (pd.to_datetime('2017-08-13') - pd.to_datetime(historical_transaction['purchase_date'])).dt.days.apply(lambda x: x if x > 0 and x < 100 else 0)\n"
      ],
      "metadata": {
        "id": "LUZx-s1II-ae"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "historical_transaction['card_id_2'] = historical_transaction['card_id']"
      ],
      "metadata": {
        "id": "tmJQbl3eJBmd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspired from https://www.kaggle.com/c/elo-merchant-category-recommendation/discussion/82055#:~:text=def%20word2vec_feature(prefix,shape))%0A%20%20%20%20return%20df_w2v\n",
        "def word2vec_feature(prefix, df, groupby, target,size):\n",
        "    df_bag = pd.DataFrame(df[[groupby, target]])\n",
        "    df_bag[target] = df_bag[target].astype(str)\n",
        "    df_bag[target].fillna('NAN', inplace=True)\n",
        "    df_bag = df_bag.groupby(groupby, as_index=False)[target].agg({'list':(lambda x: list(x))}).reset_index()\n",
        "    doc_list = list(df_bag['list'].values)\n",
        "    w2v = Word2Vec(doc_list, size=size, window=3, min_count=1, workers=32)\n",
        "    vocab_keys = list(w2v.wv.vocab.keys())\n",
        "    w2v_array = []\n",
        "    for v in vocab_keys :\n",
        "        w2v_array.append(list(w2v.wv[v]))\n",
        "    df_w2v = pd.DataFrame()\n",
        "    df_w2v['vocab_keys'] = vocab_keys    \n",
        "    df_w2v = pd.concat([df_w2v, pd.DataFrame(w2v_array)], axis=1)\n",
        "    df_w2v.columns = [target] + ['w2v_%s_%s_%d'%(prefix,target,x) for x in range(size)]\n",
        "    print ('df_w2v:' + str(df_w2v.shape))\n",
        "    return df_w2v\n",
        "\n"
      ],
      "metadata": {
        "id": "26ID-sTqL38G"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "historical_transaction['purchase_date_only'] = pd.to_datetime(historical_transaction['purchase_date']).dt.date\n",
        "card_id_word_2_vec = word2vec_feature('hist_data',historical_transaction,'card_id','card_id_2',5)\n",
        "merchant_category_id_word_2_vec = word2vec_feature('hist_data',historical_transaction,'card_id','merchant_category_id',3)\n",
        "subsector_id_word_2_vec = word2vec_feature('hist_data',historical_transaction,'card_id','subsector_id',3)\n",
        "month_lag_word_2_vec = word2vec_feature('hist_data',historical_transaction,'card_id','month_lag',3)\n",
        "purchase_date_word_2_vec = word2vec_feature('hist_data',historical_transaction,'card_id','purchase_date_only',5)\n",
        "card_id_word_2_vec.rename(columns = {'card_id_2':'card_id'}, inplace = True)\n",
        "historical_transaction = pd.merge(historical_transaction,card_id_word_2_vec,on='card_id',how='left')\n",
        "try:\n",
        "  del card_id_word_2_vec\n",
        "  print('Vector dataframe for card id is deleted')\n",
        "except:\n",
        "  pass\n",
        "merchant_category_id_word_2_vec['merchant_category_id'] = merchant_category_id_word_2_vec['merchant_category_id'].astype('int')\n",
        "historical_transaction = pd.merge(historical_transaction,merchant_category_id_word_2_vec,on='merchant_category_id',how='left')\n",
        "try:\n",
        "  del merchant_category_id_word_2_vec\n",
        "  print('Vector dataframe for merchant category id is deleted')\n",
        "except:\n",
        "  pass\n",
        "subsector_id_word_2_vec['subsector_id'] = subsector_id_word_2_vec['subsector_id'].astype('int')\n",
        "historical_transaction = pd.merge(historical_transaction,subsector_id_word_2_vec,on='subsector_id',how='left')\n",
        "try:\n",
        "  del subsector_id_word_2_vec\n",
        "  print('Vector dataframe for subsector id is deleted')\n",
        "except:\n",
        "  pass\n",
        "month_lag_word_2_vec['month_lag'] = month_lag_word_2_vec['month_lag'].astype('int')\n",
        "historical_transaction = pd.merge(historical_transaction,month_lag_word_2_vec,on='month_lag',how='left')\n",
        "try:\n",
        "  del month_lag_word_2_vec\n",
        "  print('Vector dataframe for month lag is deleted')\n",
        "except:\n",
        "  pass\n",
        "purchase_date_word_2_vec['purchase_date_only'] = pd.to_datetime(purchase_date_word_2_vec['purchase_date_only']).dt.date\n",
        "historical_transaction = pd.merge(historical_transaction,purchase_date_word_2_vec,on='purchase_date_only',how='left')\n",
        "try:\n",
        "  del purchase_date_word_2_vec\n",
        "  print('Vector dateframe for purchase date is deleted')\n",
        "except:\n",
        "  pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGPCTDjEL5rs",
        "outputId": "b234e162-880e-4a54-f801-59278434f5ae"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df_w2v:(325540, 6)\n",
            "df_w2v:(327, 4)\n",
            "df_w2v:(41, 4)\n",
            "df_w2v:(14, 4)\n",
            "df_w2v:(424, 6)\n",
            "Vector dataframe for card id is deleted\n",
            "Vector dataframe for merchant category id is deleted\n",
            "Vector dataframe for subsector id is deleted\n",
            "Vector dataframe for month lag is deleted\n",
            "Vector dateframe for purchase date is deleted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agg_policy_for_historical_transaction = {\n",
        "    'authorized_flag':['mean','sum'],\n",
        "    'city_id':['median','count','max'],\n",
        "    'category_1':['mean','sum','count','size'],\n",
        "    'installments':['mean','sum','std','min','max','var'],\n",
        "    'category_3':['mean','count','size','sum'],\n",
        "    'merchant_category_id':['median','nunique','min','max'],\n",
        "    'month_lag':['mean','sum','std','min','max','count','size','var'],\n",
        "    'purchase_amount':['mean','sum','std','min','max'],\n",
        "    'category_2':['mean','sum','max','count','size'],\n",
        "    'state_id':['nunique'],\n",
        "    'subsector_id':['nunique'],\n",
        "    'purchase_date_difference_from_today':['mean','median','min','max'],\n",
        "    'purchase_date_week_of_year':['nunique','min','max'],\n",
        "    'purchase_date_month':['nunique','sum'],\n",
        "    'purchase_date_day_of_week':['max','sum','nunique'],\n",
        "    'purchase_date_year':['nunique','sum'],\n",
        "    'Christmas_Day_2017':['mean','max','nunique','count','size'],\n",
        "    'fathers_day_2017':['mean','max','nunique','count','size'],\n",
        "    'Children_day_2017':['mean','max','nunique','count','size'],\n",
        "    'Black_Friday_2017':['mean','max','nunique','count','size'],\n",
        "    'Valentine_day_2017':['mean','max','nunique','count','size'],\n",
        "    'merchant_id':['nunique'],\n",
        "    'card_id':['size','count'],\n",
        "    'days':['mean','max','nunique','count','size'],\n",
        "    'quarter':['mean','max','nunique','count','size'],\n",
        "    'w2v_hist_data_card_id_2_0':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_card_id_2_1':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_card_id_2_2':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_card_id_2_3':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_card_id_2_4':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_merchant_category_id_0':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_merchant_category_id_1':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_merchant_category_id_2':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_subsector_id_0':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_subsector_id_1':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_subsector_id_2':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_month_lag_0':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_month_lag_1':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_month_lag_2':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_purchase_date_only_0':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_purchase_date_only_1':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_purchase_date_only_2':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_purchase_date_only_3':['sum','min','max','std','mean'],\n",
        "    'w2v_hist_data_purchase_date_only_4':['sum','min','max','std','mean'],\n",
        "    'purchase_date' : ['max','min'],\n",
        "    'month_lag' : ['max','min','mean','var','skew'],\n",
        "    'month_diff' : ['max','min','mean','var','skew'],\n",
        "    'duration' : ['mean','min','max','var','skew'],\n",
        "    'amount_month_ratio':['mean','min','max','var','skew'],\n",
        "    'price' :['sum','mean','max','min','var'],\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "aggs_table_historical_transactions = historical_transaction.groupby(['card_id']).agg(agg_policy_for_historical_transaction)\n",
        "aggs_table_historical_transactions.columns = ['hist_data_'+'_'.join(col).strip() for col in aggs_table_historical_transactions.columns.values]\n",
        "\n"
      ],
      "metadata": {
        "id": "HIhw7djZL_-R"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing full train and test dataset by merging from aggragation tables.\n",
        "train = pd.merge(train,aggs_table_historical_transactions,on='card_id',how='left')\n",
        "test = pd.merge(test,aggs_table_historical_transactions,on='card_id',how='left')\n",
        "del aggs_table_historical_transactions,historical_transaction\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdmOCQKXniYw",
        "outputId": "ef9c9b89-84b8-47d3-84a1-7a138a726deb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Refrrred from https://www.kaggle.com/roydatascience/elo-stack-with-goss-boosting\n",
        "train['hist_purchase_date_max'] = pd.to_datetime(train['hist_data_purchase_date_max'])\n",
        "train['hist_purchase_date_min'] = pd.to_datetime(train['hist_data_purchase_date_min'])\n",
        "train['hist_purchase_date_diff'] = (train['hist_purchase_date_max'] - train['hist_purchase_date_min']).dt.days\n",
        "train['hist_purchase_date_average'] = train['hist_purchase_date_diff']/train['hist_data_card_id_size']\n",
        "train['hist_purchase_date_uptonow'] = (datetime.datetime.today() - train['hist_purchase_date_max']).dt.days\n",
        "train['hist_purchase_date_uptomin'] = (datetime.datetime.today() - train['hist_purchase_date_min']).dt.days\n",
        "train['hist_first_buy'] = (train['hist_data_purchase_date_min'] - train['first_active_month']).dt.days\n",
        "train['hist_last_buy'] = (train['hist_data_purchase_date_max'] - train['first_active_month']).dt.days\n",
        "\n",
        "for feature in ['hist_data_purchase_date_max','hist_data_purchase_date_min']:\n",
        "    train[feature] = train[feature].astype(np.int64) * 1e-9\n",
        "\n",
        "test['hist_purchase_date_max'] = pd.to_datetime(test['hist_data_purchase_date_max'])\n",
        "test['hist_purchase_date_min'] = pd.to_datetime(test['hist_data_purchase_date_min'])\n",
        "test['hist_purchase_date_diff'] = (test['hist_purchase_date_max'] - test['hist_purchase_date_min']).dt.days\n",
        "test['hist_purchase_date_average'] = test['hist_purchase_date_diff']/test['hist_data_card_id_size']\n",
        "test['hist_purchase_date_uptonow'] = (datetime.datetime.today() - test['hist_purchase_date_max']).dt.days\n",
        "test['hist_purchase_date_uptomin'] = (datetime.datetime.today() - test['hist_purchase_date_min']).dt.days\n",
        "test['hist_first_buy'] = (test['hist_data_purchase_date_min'] - test['first_active_month']).dt.days\n",
        "test['hist_last_buy'] = (test['hist_data_purchase_date_max'] - test['first_active_month']).dt.days\n",
        "\n",
        "for feature in ['hist_data_purchase_date_max','hist_data_purchase_date_min']:\n",
        "    test[feature] = test[feature].astype(np.int64) * 1e-9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f57bp2-z_qzf",
        "outputId": "6cfb2c32-eadc-4d57-f45a-2f858f6c421c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Merchants Transaction"
      ],
      "metadata": {
        "id": "rAh-SiCCLtZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merchants_data = pd.read_csv('merchants.csv')\n",
        "merchants_data['category_1'] = merchants_data['category_1'].map({'Y':1,'N':0})\n",
        "merchants_data['most_recent_sales_range'] = merchants_data['most_recent_sales_range'].map({'A':1,'B':2,'C':3,'D':4,'E':5})\n",
        "merchants_data['most_recent_purchases_range'] = merchants_data['most_recent_purchases_range'].map({'A':1,'B':2,'C':3,'D':4,'E':5})\n",
        "merchants_data['category_4'] = merchants_data['category_4'].map({'Y':1,'N':0})\n",
        "\n"
      ],
      "metadata": {
        "id": "T7e9QtH2LOWU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merchants_data['merchant_id_2'] = merchants_data['merchant_id']\n",
        "merchant_id_word_2_vec = word2vec_feature('merchant_data',merchants_data,'merchant_id','merchant_id_2',5)\n",
        "merchant_id_word_2_vec.rename(columns = {'merchant_id_2':'merchant_id'}, inplace = True)\n",
        "merchants_data.drop(['merchant_id_2'],axis=1,inplace=True)\n",
        "merchants_data = pd.merge(merchants_data,merchant_id_word_2_vec,how='left',on='merchant_id')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BV-rzbpmMnwz",
        "outputId": "fa516dcd-b644-4c12-bcd3-14e0ffa25dd5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df_w2v:(334633, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merchants_data.columns = ['merchant_' + c if c not in ['merchant_id'] else c for c in merchants_data.columns ]"
      ],
      "metadata": {
        "id": "D84Goq4XsdiR"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 New merchants transactions"
      ],
      "metadata": {
        "id": "v_cewcV1Msnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_merchants_data = pd.read_csv('new_merchant_transactions.csv')\n",
        "#impute missing values\n",
        "new_merchants_data['category_2'].fillna(1.0,inplace=True)\n",
        "new_merchants_data['category_3'].fillna('A',inplace=True)\n",
        "new_merchants_data['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
        "new_merchants_data['installments'].replace(-1, np.nan,inplace=True)\n",
        "new_merchants_data['installments'].replace(999, np.nan,inplace=True)\n",
        "new_merchants_data['purchase_amount'] = new_merchants_data['purchase_amount'].apply(lambda x: min(x, 0.8))\n",
        "\n",
        "new_merchants_data['authorized_flag'] = new_merchants_data['authorized_flag'].map({'Y':1,'N':0})\n",
        "new_merchants_data['category_1'] = new_merchants_data['category_1'].map({'Y': 1, 'N': 0})\n",
        "new_merchants_data['category_3'] = new_merchants_data['category_3'].map({'A':1,'B':2,'C':3}) \n",
        "\n",
        "# new_merchants_data.columns = ['new_merchants_'+col for col in new_merchants_data.columns.values]\n",
        "# new_merchants_data['card_id'] = new_merchants_data['new_merchants_card_id']\n",
        "# new_merchants_data['merchant_id'] = new_merchants_data['new_merchants_merchant_id']\n",
        "new_merchants_data['purchase_date_difference_from_today']=abs(dt.date.today() - pd.to_datetime(new_merchants_data['purchase_date']).dt.date)\n",
        "new_merchants_data['purchase_date_difference_from_today'] = new_merchants_data['purchase_date_difference_from_today'].map(lambda x: np.nan if pd.isnull(x) else x.days)\n",
        "new_merchants_data['purchase_date_week_of_year'] = pd.to_datetime(new_merchants_data['purchase_date']).dt.isocalendar().week\n",
        "new_merchants_data['purchase_date_month'] = pd.to_datetime(new_merchants_data['purchase_date']).dt.month\n",
        "new_merchants_data['purchase_date_day_of_week'] = pd.to_datetime(new_merchants_data['purchase_date']).dt.weekday\n",
        "new_merchants_data['purchase_date_year'] = pd.to_datetime(new_merchants_data['purchase_date']).dt.year\n",
        "new_merchants_data['days'] = (dt.date(2022,1,18)-pd.to_datetime(new_merchants_data['purchase_date']).dt.date).dt.days\n",
        "new_merchants_data['quarter'] = pd.to_datetime(new_merchants_data['purchase_date']).dt.quarter"
      ],
      "metadata": {
        "id": "pAkc-J1NMq-H"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspired from https://www.kaggle.com/roydatascience/elo-stack-with-goss-boosting\n",
        "new_merchants_data['month_diff'] = new_merchants_data['days']//30\n",
        "new_merchants_data['month_diff'] += new_merchants_data['month_lag']\n",
        "new_merchants_data['duration'] = new_merchants_data['purchase_amount']*new_merchants_data['month_diff']\n",
        "new_merchants_data['amount_month_ratio'] = new_merchants_data['purchase_amount']/new_merchants_data['month_diff']\n",
        "new_merchants_data['price'] = new_merchants_data['purchase_amount'] / new_merchants_data['installments']\n"
      ],
      "metadata": {
        "id": "_RInlxtFObPr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining both merchants_data and new merchants_data\n",
        "merchants_data = pd.merge(merchants_data,new_merchants_data,on='merchant_id',how='left')"
      ],
      "metadata": {
        "id": "SfEtEQo-Q4dG"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  print('new merchants data table is deleted')\n",
        "  del new_merchants_data\n",
        "except:\n",
        "  print('new merchants data table is already deleted')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6NROOiKSYGC",
        "outputId": "b46a97bb-283f-46f8-ce79-9cc8471050ae"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new merchants data table is deleted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aggs_merchants_data = {\n",
        "    'merchant_id':['nunique'],\n",
        "    'merchant_merchant_group_id':['nunique'],\n",
        "    'merchant_merchant_category_id':['nunique'],\n",
        "    'merchant_subsector_id':['nunique'],\n",
        "    'merchant_numerical_1':['mean','sum','std','min','max','var'],\n",
        "    'merchant_numerical_2':['mean','sum','std','min','max','var'],\n",
        "    'merchant_category_1':['mean','sum','min','max','count','nunique'],\n",
        "    'merchant_most_recent_sales_range':['sum','mean','min','max','var'],\n",
        "    'merchant_most_recent_purchases_range':['sum','mean','min','max','var'],\n",
        "    'merchant_avg_sales_lag3':['mean','std','min','max','count','nunique'],\n",
        "    'merchant_avg_purchases_lag3':['mean','std','min','max','count','nunique'],\n",
        "    'merchant_active_months_lag3':['min','std','max','count','nunique','count','nunique'],\n",
        "    'merchant_avg_sales_lag6':['mean','std','min','max','count','nunique'],\n",
        "    'merchant_avg_purchases_lag6':['mean','std','min','max','count','nunique'],\n",
        "    'merchant_active_months_lag6':['min','std','max','count','nunique'],\n",
        "    'merchant_avg_sales_lag12':['mean','std','min','max','count','nunique'],\n",
        "    'merchant_avg_purchases_lag12':['mean','std','min','max','count','nunique'],\n",
        "    'merchant_active_months_lag12':['min','std','max','count','nunique'],\n",
        "    'merchant_category_4':['mean','min','max','count','nunique','std'],\n",
        "    'merchant_city_id':['nunique'],\n",
        "    'merchant_state_id':['nunique'],\n",
        "    'merchant_category_2':['mean','sum','min','max','count','nunique'],\n",
        "    'authorized_flag':['mean','sum'],\n",
        "    'card_id':['size','count'],\n",
        "    'city_id':['nunique'],\n",
        "    'merchant_merchant_category_id':['nunique'],\n",
        "    'merchant_category_1':['sum','mean','max','min','count','nunique'],\n",
        "    'installments':['sum','std','min','max'],\n",
        "    'category_3':['sum','min','max','count','nunique'],\n",
        "    'merchant_merchant_category_id':['nunique'],\n",
        "    'month_lag':['min','max','sum','count'],\n",
        "    'purchase_amount':['min','max','sum','std','var','mean'],\n",
        "    'purchase_date':['max','min'],\n",
        "    'merchant_category_2':['sum','mean','max','min'],\n",
        "    'state_id':['nunique'],\n",
        "    'subsector_id':['nunique'],\n",
        "    'purchase_date_difference_from_today':['mean','min','max'],\n",
        "    'purchase_date_week_of_year':['nunique','min','max'],\n",
        "    'purchase_date_month':['nunique','min','max'],\n",
        "    'purchase_date_day_of_week':['nunique','min','max'],\n",
        "    'purchase_date_year':['nunique','min','max'],\n",
        "    'merchant_w2v_merchant_data_merchant_id_2_0':['sum','min','max','std'],\n",
        "    'merchant_w2v_merchant_data_merchant_id_2_1':['sum','min','max','std'],\n",
        "    'merchant_w2v_merchant_data_merchant_id_2_2':['sum','min','max','std'],\n",
        "    'merchant_w2v_merchant_data_merchant_id_2_3':['sum','min','max','std'],\n",
        "    'merchant_w2v_merchant_data_merchant_id_2_4':['sum','min','max','std'],\n",
        "    'month_lag' : ['max','min','mean','var','skew'],\n",
        "    'month_diff' : ['max','min','mean','var','skew'],\n",
        "    'duration' : ['mean','min','max','var','skew'],\n",
        "    'amount_month_ratio':['mean','min','max','var','skew'],\n",
        "    'price' :['sum','mean','max','min','var'],\n",
        "    'purchase_date':['min','max']\n",
        "\n",
        "}\n",
        "# Making the aggregation table for both merchants data and new merchants_data\n",
        "agg_table_merchants_data = merchants_data.groupby('card_id').agg(aggs_merchants_data)\n",
        "agg_table_merchants_data.columns = ['merchants_data_'+'_'.join(col).strip() for col in agg_table_merchants_data.columns.values]\n"
      ],
      "metadata": {
        "id": "xQAwdRZrpchO"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing full train and test dataset by merging from aggragation tables.\n",
        "train = pd.merge(train,agg_table_merchants_data,on='card_id',how='left')\n",
        "test = pd.merge(test,agg_table_merchants_data,on='card_id',how='left')\n",
        "del agg_table_merchants_data\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3J2LHztpwSB-",
        "outputId": "696af0c5-67f2-4fcd-9552-b47e6565e5c8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Refrrred from https://www.kaggle.com/roydatascience/elo-stack-with-goss-boosting\n",
        "train['new_purchase_date_max'] = pd.to_datetime(train['merchants_data_purchase_date_year_max'])\n",
        "train['new_purchase_date_min'] = pd.to_datetime(train['merchants_data_purchase_date_year_min'])\n",
        "train['new_purchase_date_diff'] = (train['new_purchase_date_max'] - train['new_purchase_date_min']).dt.days\n",
        "train['new_purchase_date_average'] = train['new_purchase_date_diff']/train['hist_data_card_id_size']\n",
        "train['new_purchase_date_uptonow'] = (datetime.datetime.today() - train['new_purchase_date_max']).dt.days\n",
        "train['new_purchase_date_uptomin'] = (datetime.datetime.today() - train['new_purchase_date_min']).dt.days\n",
        "train['new_first_buy'] = (train['new_purchase_date_max'] - train['first_active_month']).dt.days\n",
        "train['new_last_buy'] = (train['new_purchase_date_min'] - train['first_active_month']).dt.days\n",
        "\n",
        "for feature in ['new_purchase_date_max','new_purchase_date_min']:\n",
        "    train[feature] = train[feature].astype(np.int64) * 1e-9\n",
        "\n",
        "test['new_purchase_date_max'] = pd.to_datetime(test['merchants_data_purchase_date_year_max'])\n",
        "test['new_purchase_date_min'] = pd.to_datetime(test['merchants_data_purchase_date_year_min'])\n",
        "test['new_purchase_date_diff'] = (test['new_purchase_date_max'] - test['new_purchase_date_min']).dt.days\n",
        "test['new_purchase_date_average'] = test['new_purchase_date_diff']/test['hist_data_card_id_size']\n",
        "test['new_purchase_date_uptonow'] = (datetime.datetime.today() - test['new_purchase_date_max']).dt.days\n",
        "test['new_purchase_date_uptomin'] = (datetime.datetime.today() - test['new_purchase_date_min']).dt.days\n",
        "test['new_first_buy'] = (test['new_purchase_date_max'] - test['first_active_month']).dt.days\n",
        "test['new_last_buy'] = (test['new_purchase_date_min'] - test['first_active_month']).dt.days\n",
        "\n",
        "for feature in ['new_purchase_date_max','new_purchase_date_min']:\n",
        "    test[feature] = test[feature].astype(np.int64) * 1e-9\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9xgaJj2wvuA",
        "outputId": "f0f52c2d-d16d-4259-aec4-22a5e1db254b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NEW Features referred from https://www.kaggle.com/mfjwr1/simple-lightgbm-without-blending\n",
        "train['card_id_total'] = train['merchants_data_card_id_size']+train['hist_data_card_id_size']\n",
        "train['card_id_cnt_total'] = train['merchants_data_card_id_count']+train['hist_data_card_id_count']\n",
        "train['card_id_cnt_ratio'] = train['merchants_data_card_id_count']/train['hist_data_card_id_count']\n",
        "train['purchase_amount_total'] = train['merchants_data_purchase_amount_sum']+train['hist_data_purchase_amount_sum']\n",
        "train['purchase_amount_mean'] = train['merchants_data_purchase_amount_mean']+train['hist_data_purchase_amount_mean']\n",
        "train['purchase_amount_max'] = train['merchants_data_purchase_amount_max']+train['hist_data_purchase_amount_max']\n",
        "train['purchase_amount_min'] = train['merchants_data_purchase_amount_min']+train['hist_data_purchase_amount_min']\n",
        "train['purchase_amount_ratio'] = train['merchants_data_purchase_amount_sum']/train['hist_data_purchase_amount_sum']\n",
        "train['month_diff_mean'] = train['merchants_data_month_diff_mean']+train['hist_data_month_diff_mean']\n",
        "train['month_diff_ratio'] = train['merchants_data_month_diff_mean']/train['hist_data_month_diff_mean']\n",
        "train['month_lag_mean'] = train['merchants_data_month_lag_mean']+train['hist_data_month_lag_mean']\n",
        "train['month_lag_max'] = train['merchants_data_month_lag_max']+train['hist_data_month_lag_max']\n",
        "train['month_lag_min'] = train['merchants_data_month_lag_min']+train['hist_data_month_lag_min']\n",
        "train['category_1_mean'] = train['merchants_data_merchant_category_1_mean']+train['hist_data_category_1_mean']\n",
        "train['installments_total'] = train['merchants_data_installments_sum']+train['hist_data_installments_sum']\n",
        "# train['installments_mean'] = train['merchants_data_installments_mean']+train['hist_data_installments_sum']\n",
        "train['installments_max'] = train['merchants_data_installments_max']+train['hist_data_installments_max']\n",
        "train['installments_ratio'] = train['merchants_data_installments_sum']/train['hist_data_installments_sum']\n",
        "train['price_total'] = train['hist_data_purchase_amount_sum'] / train['hist_data_installments_sum']\n",
        "train['price_mean'] = train['hist_data_purchase_amount_mean'] / train['hist_data_installments_mean']\n",
        "train['price_max'] = train['hist_data_purchase_amount_max'] / train['hist_data_installments_max']\n",
        "train['duration_mean'] = train['merchants_data_duration_mean']+train['hist_data_duration_mean']\n",
        "train['duration_min'] = train['merchants_data_duration_min']+train['hist_data_duration_min']\n",
        "train['duration_max'] = train['merchants_data_duration_max']+train['hist_data_duration_max']\n",
        "train['amount_month_ratio_mean']=train['merchants_data_amount_month_ratio_mean']+train['hist_data_amount_month_ratio_mean']\n",
        "train['amount_month_ratio_min']=train['merchants_data_amount_month_ratio_min']+train['hist_data_amount_month_ratio_min']\n",
        "train['amount_month_ratio_max']=train['merchants_data_amount_month_ratio_max']+train['hist_data_amount_month_ratio_max']\n",
        "train['new_CLV'] = train['merchants_data_card_id_count'] * train['merchants_data_purchase_amount_sum'] / train['merchants_data_month_diff_mean']\n",
        "train['hist_CLV'] = train['hist_data_card_id_count'] * train['hist_data_purchase_amount_sum'] / train['hist_data_month_diff_mean']\n",
        "train['CLV_ratio'] = train['new_CLV'] / train['hist_CLV']\n",
        "\n",
        "test['card_id_total'] = test['merchants_data_card_id_size']+test['hist_data_card_id_size']\n",
        "test['card_id_cnt_total'] = test['merchants_data_card_id_count']+test['hist_data_card_id_count']\n",
        "test['card_id_cnt_ratio'] = test['merchants_data_card_id_count']/test['hist_data_card_id_count']\n",
        "test['purchase_amount_total'] = test['merchants_data_purchase_amount_sum']+test['hist_data_purchase_amount_sum']\n",
        "test['purchase_amount_mean'] = test['merchants_data_purchase_amount_mean']+test['hist_data_purchase_amount_mean']\n",
        "test['purchase_amount_max'] = test['merchants_data_purchase_amount_max']+test['hist_data_purchase_amount_max']\n",
        "test['purchase_amount_min'] = test['merchants_data_purchase_amount_min']+test['hist_data_purchase_amount_min']\n",
        "test['purchase_amount_ratio'] = test['merchants_data_purchase_amount_sum']/test['hist_data_purchase_amount_sum']\n",
        "test['month_diff_mean'] = test['merchants_data_month_diff_mean']+test['hist_data_month_diff_mean']\n",
        "test['month_diff_ratio'] = test['merchants_data_month_diff_mean']/test['hist_data_month_diff_mean']\n",
        "test['month_lag_mean'] = test['merchants_data_month_lag_mean']+test['hist_data_month_lag_mean']\n",
        "test['month_lag_max'] = test['merchants_data_month_lag_max']+test['hist_data_month_lag_max']\n",
        "test['month_lag_min'] = test['merchants_data_month_lag_min']+test['hist_data_month_lag_min']\n",
        "test['category_1_mean'] = test['merchants_data_merchant_category_1_mean']+test['hist_data_category_1_mean']\n",
        "test['installments_total'] = test['merchants_data_installments_sum']+test['hist_data_installments_sum']\n",
        "# test['installments_mean'] = test['merchants_data_installments_mean']+test['hist_data_installments_sum']\n",
        "test['installments_max'] = test['merchants_data_installments_max']+test['hist_data_installments_max']\n",
        "test['installments_ratio'] = test['merchants_data_installments_sum']/test['hist_data_installments_sum']\n",
        "test['price_total'] = test['hist_data_purchase_amount_sum'] / test['hist_data_installments_sum']\n",
        "test['price_mean'] = test['hist_data_purchase_amount_mean'] / test['hist_data_installments_mean']\n",
        "test['price_max'] = test['hist_data_purchase_amount_max'] / test['hist_data_installments_max']\n",
        "test['duration_mean'] = test['merchants_data_duration_mean']+test['hist_data_duration_mean']\n",
        "test['duration_min'] = test['merchants_data_duration_min']+test['hist_data_duration_min']\n",
        "test['duration_max'] = test['merchants_data_duration_max']+test['hist_data_duration_max']\n",
        "test['amount_month_ratio_mean']=test['merchants_data_amount_month_ratio_mean']+test['hist_data_amount_month_ratio_mean']\n",
        "test['amount_month_ratio_min']=test['merchants_data_amount_month_ratio_min']+test['hist_data_amount_month_ratio_min']\n",
        "test['amount_month_ratio_max']=test['merchants_data_amount_month_ratio_max']+test['hist_data_amount_month_ratio_max']\n",
        "test['new_CLV'] = test['merchants_data_card_id_count'] * test['merchants_data_purchase_amount_sum'] / test['merchants_data_month_diff_mean']\n",
        "test['hist_CLV'] = test['hist_data_card_id_count'] * test['hist_data_purchase_amount_sum'] / test['hist_data_month_diff_mean']\n",
        "test['CLV_ratio'] = test['new_CLV'] / test['hist_CLV']\n"
      ],
      "metadata": {
        "id": "ZqpfIBRXMPjj"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving all these to csv files\n",
        "train.to_csv('feature_engineered_train.csv')\n",
        "test.to_csv('feature_engineered_test.csv')\n"
      ],
      "metadata": {
        "id": "_16LkUrGZ4vH"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Taken help from https://stackoverflow.com/questions/69822304/google-colab-google-drive-can%c2%b4t-be-mounted-anymore-browser-popup-google-dri\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "%cd /content\n",
        "!mkdir drive\n",
        "%cd drive\n",
        "!mkdir MyDrive\n",
        "%cd ..\n",
        "%cd ..\n",
        "!google-drive-ocamlfuse /content/drive/MyDrive\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-J6FnRU4aOcN",
        "outputId": "f6b36023-0b1e-49d7-9f0f-5f9b8602347c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 155113 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.27-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.27-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.27-0ubuntu1~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n",
            "/content\n",
            "/content/drive\n",
            "/content\n",
            "/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content')"
      ],
      "metadata": {
        "id": "naew8xEEaiEr"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv feature_engineered_train.csv /content/drive/MyDrive/appliedai/assignment/case_study_1/real_case_study_part\n",
        "!mv feature_engineered_test.csv /content/drive/MyDrive/appliedai/assignment/case_study_1/real_case_study_part"
      ],
      "metadata": {
        "id": "l5zbY7tjaie8"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4pcWqzpwb4d-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}